# æ‰‹æœºå‘å¸ƒç‰ˆæœ¬ç¼–æ’å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ - è¯¦ç»†å®ç°

åŸºäºæ‚¨çš„æ–¹æ¡ˆï¼Œæˆ‘å°†æä¾›ä¸€ä¸ªå®Œæ•´çš„ä»£ç å®ç°å’Œç›®å½•ç»“æ„ã€‚è¿™ä¸ªç³»ç»Ÿé‡‡ç”¨ç¨³å¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä¸“æ³¨äºå¤„ç†ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„ç‰ˆæœ¬å‘å¸ƒä¼˜åŒ–é—®é¢˜ã€‚

## ğŸ“ é¡¹ç›®ç›®å½•ç»“æ„

```
mobile_release_rl_system/
â”œâ”€â”€ data/                    # æ•°æ®å­˜å‚¨ç›®å½•
â”‚   â”œâ”€â”€ raw/                # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/          # å¤„ç†åçš„æ•°æ®
â”‚   â””â”€â”€ rules/              # ä¸šåŠ¡è§„åˆ™æ–‡ä»¶
â”œâ”€â”€ src/                    # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ data_loader.py      # æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
â”‚   â”œâ”€â”€ environment.py      # å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
â”‚   â”œâ”€â”€ models/             # æ¨¡å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ robust_agent.py # ç¨³å¥RLæ™ºèƒ½ä½“
â”‚   â”‚   â””â”€â”€ uncertainty.py  # ä¸ç¡®å®šæ€§æ¨¡å‹
â”‚   â”œâ”€â”€ training/           # è®­ç»ƒç›¸å…³ä»£ç 
â”‚   â”‚   â”œâ”€â”€ trainer.py      # è®­ç»ƒå™¨
â”‚   â”‚   â””â”€â”€ callback.py     # è®­ç»ƒå›è°ƒå‡½æ•°
â”‚   â”œâ”€â”€ evaluation/         # è¯„ä¼°æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ evaluator.py    # è¯„ä¼°å™¨
â”‚   â”‚   â””â”€â”€ visualizer.py   # å¯è§†åŒ–å·¥å…·
â”‚   â”œâ”€â”€ utils/              # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ helpers.py      # è¾…åŠ©å‡½æ•°
â”‚   â”‚   â””â”€â”€ config.py       # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ app/                # åº”ç”¨å±‚
â”‚       â”œâ”€â”€ api.py          # APIæ¥å£
â”‚       â”œâ”€â”€ dashboard.py    # å¯è§†åŒ–ä»ªè¡¨æ¿
â”‚       â””â”€â”€ decision.py     # å†³ç­–æ”¯æŒç³»ç»Ÿ
â”œâ”€â”€ configs/                # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ base.yaml          # åŸºç¡€é…ç½®
â”‚   â”œâ”€â”€ training.yaml      # è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ environment.yaml   # ç¯å¢ƒé…ç½®
â”œâ”€â”€ tests/                  # æµ‹è¯•ç›®å½•
â”œâ”€â”€ scripts/               # è„šæœ¬ç›®å½•
â”‚   â”œâ”€â”€ train.py          # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluate.py       # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ deploy.py         # éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md             # é¡¹ç›®è¯´æ˜
â””â”€â”€ Dockerfile            # å®¹å™¨åŒ–é…ç½®
```

## ğŸ§© æ ¸å¿ƒæ¨¡å—ä»£ç å®ç°

### 1. æ•°æ®åŠ è½½æ¨¡å— (src/data_loader.py)

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import yaml
import json
from typing import Dict, List, Tuple

class DataLoader:
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
    def load_historical_traffic(self) -> pd.DataFrame:
        """åŠ è½½å†å²æµé‡æ•°æ®"""
        df = pd.read_csv(self.config['data_paths']['historical_traffic'])
        df['date'] = pd.to_datetime(df['date'])
        df = df.set_index('date')
        return df
    
    def load_release_records(self) -> pd.DataFrame:
        """åŠ è½½ç‰ˆæœ¬å‘å¸ƒè®°å½•"""
        df = pd.read_csv(self.config['data_paths']['release_records'])
        df['release_date'] = pd.to_datetime(df['release_date'])
        return df
    
    def load_business_rules(self) -> Dict:
        """åŠ è½½ä¸šåŠ¡è§„åˆ™"""
        with open(self.config['data_paths']['business_rules'], 'r') as f:
            rules = json.load(f)
        return rules
    
    def get_holidays(self) -> List[datetime]:
        """è·å–èŠ‚å‡æ—¥åˆ—è¡¨"""
        rules = self.load_business_rules()
        holidays = [datetime.strptime(d, '%Y-%m-%d') for d in rules['holidays']]
        return holidays
    
    def preprocess_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:
        """é¢„å¤„ç†æ‰€æœ‰æ•°æ®"""
        traffic_data = self.load_historical_traffic()
        release_data = self.load_release_records()
        business_rules = self.load_business_rules()
        
        # æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹
        traffic_data = self._clean_traffic_data(traffic_data)
        release_data = self._engineer_release_features(release_data)
        
        return traffic_data, release_data, business_rules
    
    def _clean_traffic_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æµé‡æ•°æ®"""
        # å¤„ç†ç¼ºå¤±å€¼
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        # å»é™¤å¼‚å¸¸å€¼
        for col in df.columns:
            q1 = df[col].quantile(0.25)
            q3 = df[col].quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            df[col] = np.where(
                (df[col] < lower_bound) | (df[col] > upper_bound),
                df[col].median(),
                df[col]
            )
        
        return df
    
    def _engineer_release_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆç‰ˆæœ¬å‘å¸ƒç‰¹å¾"""
        # æ·»åŠ æ˜ŸæœŸå‡ ç‰¹å¾
        df['day_of_week'] = df['release_date'].dt.dayofweek
        
        # æ·»åŠ æ˜¯å¦èŠ‚å‡æ—¥ç‰¹å¾
        holidays = self.get_holidays()
        df['is_holiday'] = df['release_date'].isin(holidays).astype(int)
        
        # æ·»åŠ æœˆä»½ç‰¹å¾
        df['month'] = df['release_date'].dt.month
        
        return df
```

### 2. ç¯å¢ƒæ¨¡æ‹Ÿå™¨ (src/environment.py)

```python
import gym
from gym import spaces
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from src.models.uncertainty import UncertaintyModel

class MobileReleaseEnv(gym.Env):
    """æ‰‹æœºç‰ˆæœ¬å‘å¸ƒå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ"""
    
    def __init__(self, config: Dict, data_loader):
        super(MobileReleaseEnv, self).__init__()
        
        self.config = config
        self.data_loader = data_loader
        self.uncertainty_model = UncertaintyModel(config)
        
        # åŠ è½½æ•°æ®
        self.traffic_data, self.release_data, self.business_rules = data_loader.preprocess_data()
        
        # å®šä¹‰åŠ¨ä½œå’ŒçŠ¶æ€ç©ºé—´
        self.action_space = spaces.Discrete(2)  # 0: ä¸å‘å¸ƒ, 1: å‘å¸ƒ
        self.observation_space = self._get_observation_space()
        
        # åˆå§‹åŒ–ç¯å¢ƒçŠ¶æ€
        self.reset()
    
    def _get_observation_space(self) -> spaces.Box:
        """å®šä¹‰çŠ¶æ€ç©ºé—´"""
        # çŠ¶æ€åŒ…æ‹¬: å½“å‰å¤©æ•°, å‰©ä½™å¤©æ•°, å‘å¸ƒæ—¥å†, ç‰ˆæœ¬ä¿¡æ¯, å†å²æµé‡ç»Ÿè®¡, æµé‡è¶‹åŠ¿
        state_dim = (
            2 +  # å½“å‰å¤©æ•°å’Œå‰©ä½™å¤©æ•°
            31 +  # å‘å¸ƒæ—¥å† (31å¤©)
            5 +   # ç‰ˆæœ¬ä¿¡æ¯ (ç”¨æˆ·æ•°, åŒ…å¤§å°, å‘¨æœŸ, è¯•ç‚¹æ¯”ä¾‹, æµé‡æ¨¡å¼å‡å€¼)
            4 +   # å†å²æµé‡ç»Ÿè®¡ (å‡å€¼, æ ‡å‡†å·®, 25åˆ†ä½, 75åˆ†ä½)
            1     # æµé‡è¶‹åŠ¿ (å˜åŒ–ç‡)
        )
        return spaces.Box(low=0, high=1, shape=(state_dim,), dtype=np.float32)
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒçŠ¶æ€"""
        self.current_day = 0
        self.remaining_days = 30
        self.release_calendar = np.zeros(31, dtype=int)  # 31å¤©çš„å‘å¸ƒæ—¥å†
        self.current_version = self._get_random_version()
        self.traffic_history = self._get_initial_traffic()
        
        return self._get_state()
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›æ–°çŠ¶æ€ã€å¥–åŠ±ã€æ˜¯å¦ç»ˆæ­¢å’Œé¢å¤–ä¿¡æ¯"""
        # æ£€æŸ¥åŠ¨ä½œåˆæ³•æ€§
        is_valid, penalty = self._validate_action(action)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        if action == 1 and is_valid:
            self.release_calendar[self.current_day] = 1
            # è®¡ç®—å‘å¸ƒå¸¦æ¥çš„æµé‡å½±å“
            traffic_impact = self._calculate_traffic_impact()
            # æ›´æ–°æµé‡å†å²
            self.traffic_history = self._update_traffic_history(traffic_impact)
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€å¤©
        self.current_day += 1
        self.remaining_days -= 1
        
        # æ£€æŸ¥æ˜¯å¦ç»ˆæ­¢
        done = self.current_day >= 30 or self.remaining_days <= 0
        
        # è·å–æ–°çŠ¶æ€
        next_state = self._get_state()
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(penalty)
        
        # å‡†å¤‡é¢å¤–ä¿¡æ¯
        info = {
            'day': self.current_day,
            'action_valid': is_valid,
            'penalty': penalty,
            'traffic_impact': traffic_impact if action == 1 and is_valid else 0
        }
        
        return next_state, reward, done, info
    
    def _validate_action(self, action: int) -> Tuple[bool, float]:
        """éªŒè¯åŠ¨ä½œæ˜¯å¦åˆæ³•"""
        if action == 0:  # ä¸å‘å¸ƒæ€»æ˜¯åˆæ³•çš„
            return True, 0
        
        # æ£€æŸ¥æ˜¯å¦å‘¨æœ«
        current_date = datetime(2025, 5, 1) + timedelta(days=self.current_day)
        if current_date.weekday() >= 5:  # 5å’Œ6æ˜¯å‘¨æœ«
            return False, self.config['penalties']['weekend_release']
        
        # æ£€æŸ¥æ˜¯å¦èŠ‚å‡æ—¥
        holidays = self.data_loader.get_holidays()
        if current_date in holidays:
            return False, self.config['penalties']['holiday_release']
        
        # æ£€æŸ¥æ˜¯å¦åŒæ—¥é‡å¤å‘å¸ƒ
        if self.release_calendar[self.current_day] == 1:
            return False, self.config['penalties']['same_day_release']
        
        return True, 0
    
    def _calculate_traffic_impact(self) -> float:
        """è®¡ç®—ç‰ˆæœ¬å‘å¸ƒå¸¦æ¥çš„æµé‡å½±å“"""
        base_impact = (
            self.current_version['users'] *
            self.current_version['size_gb'] *
            self.current_version['traffic_pattern_mean']
        )
        
        # æ·»åŠ ä¸ç¡®å®šæ€§
        impact = self.uncertainty_model.apply_uncertainty(base_impact)
        
        return impact
    
    def _calculate_reward(self, penalty: float) -> float:
        """è®¡ç®—å¥–åŠ±"""
        # ç”Ÿæˆå¤šä¸ªæµé‡åœºæ™¯
        scenarios = self.uncertainty_model.generate_scenarios(
            self.traffic_history, 
            self.release_calendar,
            k=self.config['uncertainty']['num_scenarios']
        )
        
        # è®¡ç®—æ¯ä¸ªåœºæ™¯çš„æµé‡æ–¹å·®
        variances = [np.var(scenario) for scenario in scenarios]
        avg_variance = np.mean(variances)
        worst_variance = np.max(variances)
        
        # è®¡ç®—å¥–åŠ±
        reward = -(
            avg_variance +
            self.config['reward_weights']['worst_case'] * worst_variance +
            penalty
        )
        
        return reward
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€è¡¨ç¤º"""
        # å½’ä¸€åŒ–å½“å‰å¤©æ•°å’Œå‰©ä½™å¤©æ•°
        day_features = np.array([
            self.current_day / 30,
            self.remaining_days / 30
        ])
        
        # å‘å¸ƒæ—¥å†
        calendar_features = self.release_calendar / 1.0  # å·²ç»æ˜¯0æˆ–1
        
        # ç‰ˆæœ¬ä¿¡æ¯
        version_features = np.array([
            self.current_version['users'] / self.config['normalization']['max_users'],
            self.current_version['size_gb'] / self.config['normalization']['max_size_gb'],
            self.current_version['period'] / self.config['normalization']['max_period'],
            self.current_version['pilot_ratio'],
            self.current_version['traffic_pattern_mean'] / self.config['normalization']['max_traffic_pattern']
        ])
        
        # å†å²æµé‡ç»Ÿè®¡
        traffic_stats = np.array([
            np.mean(self.traffic_history),
            np.std(self.traffic_history),
            np.percentile(self.traffic_history, 25),
            np.percentile(self.traffic_history, 75)
        ]) / self.config['normalization']['max_traffic']
        
        # æµé‡è¶‹åŠ¿
        if len(self.traffic_history) >= 14:
            recent_mean = np.mean(self.traffic_history[-7:])
            previous_mean = np.mean(self.traffic_history[-14:-7])
            trend = (recent_mean - previous_mean) / previous_mean if previous_mean > 0 else 0
        else:
            trend = 0
        
        trend_feature = np.array([(trend + 1) / 2])  # å½’ä¸€åŒ–åˆ°[0,1]
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        state = np.concatenate([
            day_features,
            calendar_features,
            version_features,
            traffic_stats,
            trend_feature
        ])
        
        return state
    
    def _get_random_version(self) -> Dict:
        """è·å–éšæœºç‰ˆæœ¬ä¿¡æ¯"""
        # ä»å‘å¸ƒè®°å½•ä¸­éšæœºé€‰æ‹©ä¸€æ¡è®°å½•
        idx = np.random.randint(0, len(self.release_data))
        version = self.release_data.iloc[idx].to_dict()
        
        return version
    
    def _get_initial_traffic(self) -> List[float]:
        """è·å–åˆå§‹æµé‡å†å²"""
        # åŸºäºå†å²æ•°æ®è·å–åˆå§‹æµé‡
        start_date = datetime(2025, 5, 1) - timedelta(days=14)
        end_date = datetime(2025, 4, 30)
        
        # è·å–å†å²æµé‡æ•°æ®
        historical_traffic = self.traffic_data.loc[start_date:end_date]
        
        return historical_traffic['traffic'].tolist()
    
    def _update_traffic_history(self, impact: float) -> List[float]:
        """æ›´æ–°æµé‡å†å²"""
        # æ·»åŠ æ–°ä¸€å¤©çš„æµé‡ï¼ˆåŸºäºå†å²æ•°æ®å’Œå‘å¸ƒå½±å“ï¼‰
        base_traffic = self.traffic_data.loc[
            datetime(2025, 5, 1) + timedelta(days=self.current_day)
        ]['traffic']
        
        new_traffic = base_traffic + impact
        self.traffic_history.append(new_traffic)
        
        # ä¿æŒå›ºå®šé•¿åº¦çš„å†å²çª—å£
        if len(self.traffic_history) > self.config['traffic_history_window']:
            self.traffic_history = self.traffic_history[-self.config['traffic_history_window']:]
        
        return self.traffic_history
```

### 3. ä¸ç¡®å®šæ€§æ¨¡å‹ (src/models/uncertainty.py)

```python
import numpy as np
from typing import List
import scipy.stats as stats

class UncertaintyModel:
    """ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå¤šåœºæ™¯æµé‡æ¨¡æ‹Ÿ"""
    
    def __init__(self, config: Dict):
        self.config = config
        
    def generate_scenarios(self, historical_traffic: List[float], 
                          release_calendar: np.ndarray, k: int = 100) -> List[List[float]]:
        """ç”ŸæˆKä¸ªå¯èƒ½çš„æœªæ¥æµé‡åœºæ™¯"""
        scenarios = []
        
        for _ in range(k):
            scenario = self._generate_single_scenario(historical_traffic, release_calendar)
            scenarios.append(scenario)
        
        return scenarios
    
    def _generate_single_scenario(self, historical_traffic: List[float], 
                                release_calendar: np.ndarray) -> List[float]:
        """ç”Ÿæˆå•ä¸ªæµé‡åœºæ™¯"""
        scenario = []
        current_traffic = historical_traffic[-1] if historical_traffic else 0
        
        for day in range(len(release_calendar)):
            # åŸºç¡€æµé‡ï¼ˆåŸºäºå†å²åŒæœŸï¼‰
            base_flow = self._get_historical_baseline(day)
            
            # éšæœºæ³¢åŠ¨
            random_factor = np.random.normal(1.0, self.config['uncertainty']['random_std'])
            
            # ç‰ˆæœ¬å‘å¸ƒå½±å“
            release_impact = 0
            if release_calendar[day] == 1:
                release_impact = self._simulate_release_impact(day)
            
            # ç»„åˆæ‰€æœ‰å› ç´ 
            daily_traffic = base_flow * random_factor + release_impact
            scenario.append(daily_traffic)
            
            # æ›´æ–°å½“å‰æµé‡ï¼ˆå¸¦æœ‰å¹³æ»‘æ•ˆåº”ï¼‰
            current_traffic = current_traffic * 0.7 + daily_traffic * 0.3
        
        return scenario
    
    def _get_historical_baseline(self, day: int) -> float:
        """è·å–å†å²åŒæœŸæµé‡åŸºçº¿"""
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”æ ¹æ®å†å²æ•°æ®è®¡ç®—
        # å¯ä»¥ä½¿ç”¨ç§»åŠ¨å¹³å‡ã€å­£èŠ‚æ€§åˆ†è§£ç­‰æ–¹æ³•
        baseline = 1000  # é»˜è®¤åŸºçº¿å€¼
        
        # æ·»åŠ æ˜ŸæœŸæ•ˆåº”
        day_of_week = (day + 3) % 7  # å‡è®¾5æœˆ1æ—¥æ˜¯æ˜ŸæœŸå››
        if day_of_week >= 5:  # å‘¨æœ«æ•ˆåº”
            baseline *= 1.2
        
        return baseline
    
    def _simulate_release_impact(self, day: int) -> float:
        """æ¨¡æ‹Ÿç‰ˆæœ¬å‘å¸ƒå¸¦æ¥çš„æµé‡å½±å“"""
        # åŸºç¡€å½±å“
        base_impact = np.random.lognormal(
            self.config['release_impact']['log_mean'],
            self.config['release_impact']['log_std']
        )
        
        # è¡°å‡å› å­ï¼ˆéšæ—¶é—´è¡°å‡ï¼‰
        decay = 1.0
        days_after_release = 0
        
        # è®¡ç®—è¡°å‡
        if days_after_release > 0:
            decay = np.exp(-days_after_release / self.config['release_impact']['decay_rate'])
        
        return base_impact * decay
    
    def apply_uncertainty(self, base_value: float) -> float:
        """å¯¹åŸºç¡€å€¼åº”ç”¨ä¸ç¡®å®šæ€§"""
        # ä½¿ç”¨æ­£æ€åˆ†å¸ƒæ·»åŠ éšæœºæ‰°åŠ¨
        perturbation = np.random.normal(1.0, self.config['uncertainty']['perturbation_std'])
        
        return base_value * perturbation
```

### 4. ç¨³å¥RLæ™ºèƒ½ä½“ (src/models/robust_agent.py)

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import numpy as np

class RobustPolicyNetwork(nn.Module):
    """ç¨³å¥ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(RobustPolicyNetwork, self).__init__()
        
        self.shared_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
    def forward(self, x):
        shared_features = self.shared_net(x)
        action_logits = self.actor(shared_features)
        state_value = self.critic(shared_features)
        
        return action_logits, state_value

class RobustPPOAgent:
    """ç¨³å¥PPOæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim, action_dim, config):
        self.config = config
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        self.policy = RobustPolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=config['learning_rate'])
        
        self.memory = {
            'states': [],
            'actions': [],
            'log_probs': [],
            'rewards': [],
            'values': [],
            'dones': []
        }
        
    def select_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            action_logits, state_value = self.policy(state_tensor)
        
        action_probs = F.softmax(action_logits, dim=-1)
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        
        # å­˜å‚¨ç»éªŒ
        self.memory['states'].append(state)
        self.memory['actions'].append(action.item())
        self.memory['log_probs'].append(log_prob.item())
        self.memory['values'].append(state_value.item())
        
        return action.item()
    
    def update(self):
        """æ›´æ–°ç­–ç•¥"""
        # è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
        returns = self._compute_returns()
        advantages = self._compute_advantages(returns)
        
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor(np.array(self.memory['states']))
        actions = torch.LongTensor(np.array(self.memory['actions']))
        old_log_probs = torch.FloatTensor(np.array(self.memory['log_probs']))
        returns = torch.FloatTensor(returns)
        advantages = torch.FloatTensor(advantages)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPOæ›´æ–°
        for _ in range(self.config['ppo_epochs']):
            # è·å–æ–°ç­–ç•¥çš„è¾“å‡º
            action_logits, state_values = self.policy(states)
            action_probs = F.softmax(action_logits, dim=-1)
            dist = Categorical(action_probs)
            
            # è®¡ç®—æ–°logæ¦‚ç‡å’Œç†µ
            new_log_probs = dist.log_prob(actions)
            entropy = dist.entropy().mean()
            
            # è®¡ç®—ç­–ç•¥æ¯”ç‡
            ratios = torch.exp(new_log_probs - old_log_probs)
            
            # è®¡ç®—ç­–ç•¥æŸå¤±
            surr1 = ratios * advantages
            surr2 = torch.clamp(
                ratios, 
                1 - self.config['clip_epsilon'], 
                1 + self.config['clip_epsilon']
            ) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # è®¡ç®—ä»·å€¼æŸå¤±
            value_loss = F.mse_loss(state_values.squeeze(), returns)
            
            # æ€»æŸå¤±
            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
            self.optimizer.step()
        
        # æ¸…ç©ºè®°å¿†
        self._clear_memory()
    
    def _compute_returns(self):
        """è®¡ç®—å›æŠ¥"""
        returns = []
        discounted_reward = 0
        
        for reward, is_terminal in zip(reversed(self.memory['rewards']), 
                                      reversed(self.memory['dones'])):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.config['gamma'] * discounted_reward)
            returns.insert(0, discounted_reward)
        
        return returns
    
    def _compute_advantages(self, returns):
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•°"""
        advantages = []
        values = self.memory['values']
        
        for i in range(len(returns)):
            advantage = returns[i] - values[i]
            advantages.append(advantage)
        
        return advantages
    
    def _clear_memory(self):
        """æ¸…ç©ºç»éªŒè®°å¿†"""
        for key in self.memory:
            self.memory[key] = []
    
    def save(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy_state_dict': self.policy.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict()
        }, path)
    
    def load(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path)
        self.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

### 5. è®­ç»ƒå™¨ (src/training/trainer.py)

```python
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import os

class PPOTrainer:
    """PPOè®­ç»ƒå™¨"""
    
    def __init__(self, env, agent, config):
        self.env = env
        self.agent = agent
        self.config = config
        self.stats = {
            'episode_rewards': [],
            'episode_lengths': [],
            'value_losses': [],
            'policy_losses': []
        }
    
    def train(self, num_episodes):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        for episode in tqdm(range(num_episodes)):
            state = self.env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.select_action(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = self.env.step(action)
                
                # å­˜å‚¨ç»éªŒ
                self.agent.memory['rewards'].append(reward)
                self.agent.memory['dones'].append(done)
                
                # æ›´æ–°çŠ¶æ€
                state = next_state
                episode_reward += reward
            
            # æ›´æ–°ç­–ç•¥
            self.agent.update()
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            self.stats['episode_rewards'].append(episode_reward)
            self.stats['episode_lengths'].append(len(self.agent.memory['rewards']))
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if episode % self.config['save_interval'] == 0:
                self.agent.save(f"checkpoints/agent_episode_{episode}.pt")
            
            # æ‰“å°è¿›åº¦
            if episode % self.config['log_interval'] == 0:
                avg_reward = np.mean(self.stats['episode_rewards'][-100:])
                print(f"Episode {episode}, Average Reward: {avg_reward:.2f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.agent.save("checkpoints/agent_final.pt")
        
        return self.stats
    
    def plot_training_progress(self):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(self.stats['episode_rewards'])
        axes[0, 0].set_title('Episode Rewards')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        
        # æ»‘åŠ¨å¹³å‡å¥–åŠ±
        window_size = 100
        moving_avg = np.convolve(
            self.stats['episode_rewards'], 
            np.ones(window_size)/window_size, 
            mode='valid'
        )
        axes[0, 1].plot(moving_avg)
        axes[0, 1].set_title('Moving Average (100 episodes)')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Average Reward')
        
        # å›åˆé•¿åº¦
        axes[1, 0].plot(self.stats['episode_lengths'])
        axes[1, 0].set_title('Episode Lengths')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Length')
        
        # æŸå¤±æ›²çº¿
        if self.stats['value_losses']:
            axes[1, 1].plot(self.stats['value_losses'], label='Value Loss')
        if self.stats['policy_losses']:
            axes[1, 1].plot(self.stats['policy_losses'], label='Policy Loss')
        axes[1, 1].set_title('Training Losses')
        axes[1, 1].set_xlabel('Update Step')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig('training_progress.png')
        plt.show()
```

### 6. é…ç½®æ–‡ä»¶ (configs/base.yaml)

```yaml
# æ•°æ®è·¯å¾„é…ç½®
data_paths:
  historical_traffic: "data/raw/historical_traffic.csv"
  release_records: "data/raw/release_records.csv"
  business_rules: "data/rules/business_rules.json"

# ç¯å¢ƒé…ç½®
environment:
  traffic_history_window: 30
  penalties:
    weekend_release: 10.0
    holiday_release: 10.0
    same_day_release: 5.0

# ä¸ç¡®å®šæ€§æ¨¡å‹é…ç½®
uncertainty:
  num_scenarios: 100
  random_std: 0.1
  perturbation_std: 0.3

# å‘å¸ƒå½±å“é…ç½®
release_impact:
  log_mean: 0.0
  log_std: 0.5
  decay_rate: 3.0

# å¥–åŠ±æƒé‡é…ç½®
reward_weights:
  worst_case: 2.0

# å½’ä¸€åŒ–é…ç½®
normalization:
  max_users: 1000000
  max_size_gb: 5.0
  max_period: 30
  max_traffic_pattern: 1.0
  max_traffic: 10000

# è®­ç»ƒé…ç½®
training:
  learning_rate: 0.0003
  gamma: 0.99
  clip_epsilon: 0.2
  ppo_epochs: 4
  batch_size: 64
  num_episodes: 10000
  save_interval: 1000
  log_interval: 100
```

### 7. ä¸»è®­ç»ƒè„šæœ¬ (scripts/train.py)

```python
import yaml
from src.data_loader import DataLoader
from src.environment import MobileReleaseEnv
from src.models.robust_agent import RobustPPOAgent
from src.training.trainer import PPOTrainer

def main():
    # åŠ è½½é…ç½®
    with open('configs/base.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
    data_loader = DataLoader('configs/base.yaml')
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    env = MobileReleaseEnv(config, data_loader)
    
    # åˆå§‹åŒ–æ™ºèƒ½ä½“
    agent = RobustPPOAgent(
        state_dim=env.observation_space.shape[0],
        action_dim=env.action_space.n,
        config=config['training']
    )
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = PPOTrainer(env, agent, config['training'])
    
    # å¼€å§‹è®­ç»ƒ
    stats = trainer.train(config['training']['num_episodes'])
    
    # ç»˜åˆ¶è®­ç»ƒè¿›åº¦
    trainer.plot_training_progress()
    
    print("è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main()
```

### 8. è¯„ä¼°è„šæœ¬ (scripts/evaluate.py)

```python
import yaml
import numpy as np
from src.data_loader import DataLoader
from src.environment import MobileReleaseEnv
from src.models.robust_agent import RobustPPOAgent
from src.evaluation.evaluator import RobustnessEvaluator

def main():
    # åŠ è½½é…ç½®
    with open('configs/base.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
    data_loader = DataLoader('configs/base.yaml')
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    env = MobileReleaseEnv(config, data_loader)
    
    # åˆå§‹åŒ–æ™ºèƒ½ä½“
    agent = RobustPPOAgent(
        state_dim=env.observation_space.shape[0],
        action_dim=env.action_space.n,
        config=config['training']
    )
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    agent.load("checkpoints/agent_final.pt")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = RobustnessEvaluator(env, agent, config)
    
    # è¯„ä¼°ç¨³å¥æ€§
    results = evaluator.evaluate_robustness()
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    print("ç¨³å¥æ€§è¯„ä¼°ç»“æœ:")
    print(f"å¹³å‡æµé‡æ–¹å·®: {results['avg_variance']:.2f}")
    print(f"æœ€åæƒ…å†µæ–¹å·®: {results['worst_variance']:.2f}")
    print(f"è¿è§„æ¬¡æ•°: {results['violation_count']}")
    print(f"å¹³å‡å¥–åŠ±: {results['avg_reward']:.2f}")
    
    # å¯è§†åŒ–è¯„ä¼°ç»“æœ
    evaluator.visualize_results(results)

if __name__ == "__main__":
    main()
```

## ğŸš€ éƒ¨ç½²å’Œè¿è¡ŒæŒ‡å—

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. å‡†å¤‡æ•°æ®

å°†å†å²æµé‡æ•°æ®ã€ç‰ˆæœ¬å‘å¸ƒè®°å½•å’Œä¸šåŠ¡è§„åˆ™æ–‡ä»¶æ”¾ç½®åœ¨ç›¸åº”çš„ç›®å½•ä¸­ï¼š
- `data/raw/historical_traffic.csv`
- `data/raw/release_records.csv`
- `data/rules/business_rules.json`

### 3. è®­ç»ƒæ¨¡å‹

```bash
python scripts/train.py
```

### 4. è¯„ä¼°æ¨¡å‹

```bash
python scripts/evaluate.py
```

### 5. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå†³ç­–

```python
from src.app.decision import DecisionSupportSystem

# åˆå§‹åŒ–å†³ç­–æ”¯æŒç³»ç»Ÿ
dss = DecisionSupportSystem("checkpoints/agent_final.pt", "configs/base.yaml")

# è·å–å‘å¸ƒå»ºè®®
recommendation = dss.get_release_recommendation(current_state)

# å¯è§†åŒ–ç»“æœ
dss.visualize_recommendation(recommendation)
```

## ğŸ“Š é¢„æœŸè¾“å‡º

ç³»ç»Ÿå°†ç”Ÿæˆä»¥ä¸‹è¾“å‡ºï¼š
1. è®­ç»ƒè¿›åº¦å›¾è¡¨ï¼ˆå¥–åŠ±æ›²çº¿ã€æŸå¤±æ›²çº¿ç­‰ï¼‰
2. ç¨³å¥æ€§è¯„ä¼°æŠ¥å‘Šï¼ˆå¹³å‡æ–¹å·®ã€æœ€åæƒ…å†µæ–¹å·®ç­‰æŒ‡æ ‡ï¼‰
3. å‘å¸ƒæ—¥å†å¯è§†åŒ–ï¼ˆæ˜¾ç¤ºæ¨èçš„å‘å¸ƒæ—¥æœŸï¼‰
4. æµé‡é¢„æµ‹å›¾è¡¨ï¼ˆæ˜¾ç¤ºé¢„æµ‹çš„æµé‡æ¨¡å¼ï¼‰
5. å†³ç­–è§£é‡ŠæŠ¥å‘Šï¼ˆè¯´æ˜å†³ç­–çš„å…³é”®å› ç´ ï¼‰

## ğŸ”§ è‡ªå®šä¹‰å’Œæ‰©å±•

ç³»ç»Ÿè®¾è®¡ä¸ºé«˜åº¦å¯é…ç½®å’Œå¯æ‰©å±•çš„ï¼š

1. **ä¿®æ”¹é…ç½®**ï¼šé€šè¿‡ç¼–è¾‘YAMLé…ç½®æ–‡ä»¶è°ƒæ•´ç®—æ³•å‚æ•°å’Œä¸šåŠ¡è§„åˆ™
2. **æ·»åŠ æ–°ç‰¹å¾**ï¼šåœ¨ç¯å¢ƒçš„çŠ¶æ€è¡¨ç¤ºä¸­æ·»åŠ æ–°çš„ç‰¹å¾ç»´åº¦
3. **æ‰©å±•ä¸ç¡®å®šæ€§æ¨¡å‹**ï¼šå®ç°æ›´å¤æ‚çš„ä¸ç¡®å®šæ€§æ¨¡æ‹Ÿæ–¹æ³•
4. **é›†æˆæ–°ç®—æ³•**ï¼šé€šè¿‡å®ç°ç»Ÿä¸€çš„æ¥å£é›†æˆå…¶ä»–å¼ºåŒ–å­¦ä¹ ç®—æ³•
5. **æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡**ï¼šåœ¨è¯„ä¼°å™¨ä¸­æ·»åŠ æ–°çš„æ€§èƒ½æŒ‡æ ‡

è¿™ä¸ªå®ç°æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„æ‰‹æœºç‰ˆæœ¬å‘å¸ƒç¼–æ’å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œä¸“æ³¨äºå¤„ç†ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„å†³ç­–ä¼˜åŒ–é—®é¢˜ã€‚ç³»ç»Ÿé‡‡ç”¨äº†ç¨³å¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æµé‡é¢„æµ‹ä¸å‡†ç¡®çš„æƒ…å†µä¸‹ä¾ç„¶ç”Ÿæˆç¨³å®šçš„å‘å¸ƒæ–¹æ¡ˆã€‚