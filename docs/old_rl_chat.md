å¥½çš„ âœ… æˆ‘ä¼šæŠŠ **â€œæ‰‹æœºå‘å¸ƒç‰ˆæœ¬ç¼–æ’å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼ˆç¨³å¥æ€§ä¼˜åŒ–ä¸ä¸ç¡®å®šæ€§å»ºæ¨¡æŠ€æœ¯æ–¹æ¡ˆï¼‰â€** é‡æ–°è®¾è®¡æˆè¯¦ç»†çš„ä»£ç å®ç°æ–¹æ¡ˆï¼Œå¹¶ä¸”æ‹†åˆ†ä¸º **3 æ‰¹æ¬¡** è¾“å‡ºã€‚
ç¬¬ä¸€æ‰¹æ¬¡æˆ‘å…ˆç»™ä½ ï¼š**ä»£ç ç›®å½•ç»“æ„ + æ•°æ®å±‚ & ç¯å¢ƒæ¨¡æ‹Ÿå™¨å®ç°**ã€‚
åç»­ç¬¬äºŒæ‰¹æ¬¡ä¼šç»™ä½  **ä¸ç¡®å®šæ€§å»ºæ¨¡ + å¥–åŠ±å‡½æ•° + RL æ™ºèƒ½ä½“**ï¼Œç¬¬ä¸‰æ‰¹æ¬¡ä¼šç»™ä½  **è®­ç»ƒæµç¨‹ + åº”ç”¨å±‚æ¥å£ + å¯è§†åŒ– & è¯„ä¼°æ¨¡å—**ã€‚

---

# ğŸ“‚ ç¬¬ä¸€æ‰¹æ¬¡è¾“å‡º

## 1. ä»£ç ç›®å½•è®¾è®¡

```
rl_release_planner/
â”œâ”€â”€ data_layer/                # æ•°æ®å±‚
â”‚   â”œâ”€â”€ data_loader.py         # å†å²æµé‡ã€ç‰ˆæœ¬ã€è§„åˆ™æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ holiday_rules.py       # èŠ‚å‡æ—¥ä¸è§„åˆ™çº¦æŸ
â”‚   â””â”€â”€ preprocess.py          # ç‰¹å¾æå–ä¸å½’ä¸€åŒ–
â”‚
â”œâ”€â”€ env/                       # ç¯å¢ƒæ¨¡æ‹Ÿå™¨
â”‚   â”œâ”€â”€ release_env.py         # ä¸»ç¯å¢ƒç±»ï¼ŒçŠ¶æ€è½¬ç§» & å¥–åŠ±
â”‚   â”œâ”€â”€ traffic_simulator.py   # æµé‡åŸºçº¿ä¸æ‰°åŠ¨æ¨¡æ‹Ÿ
â”‚   â”œâ”€â”€ version_effect.py      # ç‰ˆæœ¬å‘å¸ƒå¯¹æµé‡å½±å“å»ºæ¨¡
â”‚   â””â”€â”€ utils.py               # é€šç”¨å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ agent/                     # å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
â”‚   â”œâ”€â”€ ppo_agent.py           # PPO å®ç°ï¼ˆç¨³å®šè®­ç»ƒï¼‰
â”‚   â”œâ”€â”€ memory.py              # ç»éªŒå›æ”¾ç¼“å†²åŒº
â”‚   â””â”€â”€ model.py               # ç­–ç•¥ç½‘ç»œ & å€¼å‡½æ•°ç½‘ç»œ
â”‚
â”œâ”€â”€ evaluation/                # è¯„ä¼°ä¸å¯¹æ¯”
â”‚   â”œâ”€â”€ evaluator.py           # ç­–ç•¥è¯„ä¼°ï¼Œå¤šåœºæ™¯æ¨¡æ‹Ÿ
â”‚   â”œâ”€â”€ baselines.py           # ä¸ GA / äººå·¥ç­–ç•¥å¯¹æ¯”
â”‚   â””â”€â”€ visualization.py       # æ›²çº¿ã€å‘å¸ƒæ—¥å†å¯è§†åŒ–
â”‚
â”œâ”€â”€ app/                       # åº”ç”¨å±‚æ¥å£
â”‚   â”œâ”€â”€ api.py                 # REST API / gRPC æ¥å£
â”‚   â”œâ”€â”€ dashboard.py           # å†³ç­–å¯è§†åŒ–ç•Œé¢
â”‚   â””â”€â”€ planner.py             # å†³ç­–æ”¯æŒç³»ç»Ÿ
â”‚
â”œâ”€â”€ configs/                   # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ env_config.yaml        # ç¯å¢ƒå‚æ•° (Î², K, æœ€å¤§å¤©æ•°ç­‰)
â”‚   â””â”€â”€ agent_config.yaml      # è®­ç»ƒè¶…å‚æ•°
â”‚
â”œâ”€â”€ tests/                     # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_env.py
â”‚   â”œâ”€â”€ test_agent.py
â”‚   â””â”€â”€ test_evaluator.py
â”‚
â”œâ”€â”€ main_train.py              # ä¸»è®­ç»ƒå…¥å£
â”œâ”€â”€ main_eval.py               # è¯„ä¼°å…¥å£
â””â”€â”€ requirements.txt           # ä¾èµ–åº“
```

---

## 2. æ•°æ®å±‚å®ç°

### `data_loader.py`

```python
import pandas as pd

class DataLoader:
    def __init__(self, traffic_path, version_path, rules_path):
        self.traffic_path = traffic_path
        self.version_path = version_path
        self.rules_path = rules_path

    def load_traffic(self):
        df = pd.read_csv(self.traffic_path, parse_dates=["date"])
        return df

    def load_versions(self):
        df = pd.read_csv(self.version_path, parse_dates=["release_date"])
        return df

    def load_rules(self):
        df = pd.read_csv(self.rules_path, parse_dates=["date"])
        return df
```

---

### `holiday_rules.py`

```python
import datetime

class HolidayRules:
    def __init__(self, holidays, weekends=True):
        self.holidays = set(holidays)
        self.weekends = weekends

    def is_valid_release(self, date):
        if self.weekends and date.weekday() >= 5:
            return False
        if date in self.holidays:
            return False
        return True
```

---

### `preprocess.py`

```python
import numpy as np

def normalize_feature(x, min_val, max_val):
    return (x - min_val) / (max_val - min_val + 1e-8)

def extract_features(traffic_df, window=7):
    """
    è¾“å…¥: å†å²æµé‡æ•°æ®
    è¾“å‡º: ç‰¹å¾ (å‡å€¼, æ ‡å‡†å·®, åˆ†ä½æ•°, è¶‹åŠ¿)
    """
    values = traffic_df["traffic"].values
    mean = np.mean(values[-window:])
    std = np.std(values[-window:])
    q25 = np.percentile(values[-window:], 25)
    q75 = np.percentile(values[-window:], 75)

    trend = (np.mean(values[-window:]) - np.mean(values[-2*window:-window])) / (np.mean(values[-2*window:-window]) + 1e-8)

    return np.array([mean, std, q25, q75, trend])
```

---

## 3. ç¯å¢ƒæ¨¡æ‹Ÿå™¨å®ç°

### `traffic_simulator.py`

```python
import numpy as np
import random

class TrafficSimulator:
    def __init__(self, base_traffic_series):
        self.base = base_traffic_series

    def sample_future(self, horizon=31, scenarios=100):
        """
        ç”Ÿæˆå¤šåœºæ™¯æµé‡è½¨è¿¹
        """
        scenarios_list = []
        for _ in range(scenarios):
            noise = np.random.normal(1.0, 0.1, size=horizon)  # éšæœºæ‰°åŠ¨
            scenario = self.base[:horizon] * noise
            scenarios_list.append(scenario)
        return scenarios_list
```

---

### `version_effect.py`

```python
import numpy as np

class VersionEffect:
    def __init__(self, duration=5, decay=0.8):
        self.duration = duration
        self.decay = decay

    def apply(self, traffic_series, day, version_info):
        """
        ç‰ˆæœ¬å‘å¸ƒå¯¹æµé‡çš„å½±å“
        """
        base_effect = version_info["users"] * version_info["pkg_size"] * version_info["mean_traffic"]
        disturbance = np.random.normal(1.0, 0.3)

        for i in range(self.duration):
            if day + i < len(traffic_series):
                traffic_series[day + i] += base_effect * (self.decay ** i) * disturbance

        return traffic_series
```

---

### `release_env.py`

```python
import numpy as np
from env.traffic_simulator import TrafficSimulator
from env.version_effect import VersionEffect

class ReleaseEnv:
    def __init__(self, traffic_baseline, rules, versions, horizon=31, scenarios=100):
        self.traffic_baseline = traffic_baseline
        self.rules = rules
        self.versions = versions
        self.horizon = horizon
        self.scenarios = scenarios
        self.reset()

    def reset(self):
        self.day = 0
        self.calendar = np.zeros(self.horizon, dtype=int)
        self.traffic = np.copy(self.traffic_baseline)
        return self._get_state()

    def _get_state(self):
        return {
            "day": self.day,
            "calendar": self.calendar.copy(),
            "traffic": self.traffic[:self.day+1]
        }

    def step(self, action, version_info):
        reward = 0
        done = False

        if action == 1:  # å‘å¸ƒç‰ˆæœ¬
            if not self.rules.is_valid_release(self.day):
                reward -= 100  # éæ³•æƒ©ç½š
            else:
                ve = VersionEffect()
                self.traffic = ve.apply(self.traffic, self.day, version_info)
                self.calendar[self.day] = 1
                reward += 1  # åŸºç¡€å¥–åŠ±

        self.day += 1
        if self.day >= self.horizon:
            done = True

        return self._get_state(), reward, done
```

---

ğŸ“Œ **åˆ°è¿™é‡Œï¼šç¬¬ä¸€æ‰¹æ¬¡** å·²ç»åŒ…å«äº† **æ•°æ®å±‚ï¼ˆæ•°æ®åŠ è½½ã€è§„åˆ™æ ¡éªŒã€ç‰¹å¾å¤„ç†ï¼‰** + **ç¯å¢ƒæ¨¡æ‹Ÿå™¨ï¼ˆæµé‡æ¨¡æ‹Ÿã€ç‰ˆæœ¬å½±å“ã€ç¯å¢ƒäº¤äº’ï¼‰** çš„å®Œæ•´å®ç°ã€‚

è¦ä¸è¦æˆ‘ç°åœ¨ç»§ç»­è¾“å‡º **ç¬¬äºŒæ‰¹æ¬¡ï¼ˆä¸ç¡®å®šæ€§å»ºæ¨¡ + å¥–åŠ±å‡½æ•° + RL æ™ºèƒ½ä½“å®ç°ï¼‰**ï¼Ÿ

ä¸‹é¢ç»™å‡º**ç¬¬äºŒæ‰¹æ¬¡ä»£ç **ï¼ˆä¸ç¡®å®šæ€§å»ºæ¨¡ + å¥–åŠ±å‡½æ•° + RL æ™ºèƒ½ä½“å®ç°ï¼‰ã€‚
æœ¬æ‰¹æ¬¡é»˜è®¤å¯ä»¥ç›´æ¥æ¥å…¥ä½ ç¬¬ä¸€æ‰¹æ¬¡ä¸­çš„â€œç¯å¢ƒçŠ¶æ€ä¸è§„åˆ™æ ¡éªŒæ¥å£â€ï¼Œä¹Ÿå†…ç½®äº†ä¸€ä¸ªæœ€å°å¯è¿è¡Œçš„ç¤ºä¾‹ç¯å¢ƒ `ReleaseEnv` ä¾¿äºè”è°ƒä¸å•æµ‹ã€‚

---

# ä»£ç ç›®å½•

```
rl_core/
  __init__.py
  uncertainty/
    __init__.py
    scenarios.py          # ä¸ç¡®å®šæ€§/å¤šåœºæ™¯é‡‡æ ·ä¸æç«¯æ‰°åŠ¨
  reward/
    __init__.py
    reward.py             # 95åˆ†ä½/æ–¹å·®å‹ç¨³å¥å¥–åŠ± + è¿è§„æƒ©ç½š
  agent/
    __init__.py
    networks.py           # ç­–ç•¥/ä»·å€¼ç½‘ç»œ
    buffers.py            # PPOç»éªŒç¼“å†²
    ppo.py                # PPO-Clip æ™ºèƒ½ä½“ï¼ˆå«é²æ£’è®­ç»ƒå¾ªç¯ï¼‰
  envs/
    __init__.py
    release_env.py        # æœ€å°å¯è¿è¡Œç¤ºä¾‹ç¯å¢ƒï¼ˆè°ƒç”¨ä¸ç¡®å®šæ€§ä¸å¥–åŠ±ï¼‰
  utils/
    __init__.py
    seeding.py            # éšæœºç§å­ç»Ÿä¸€æ§åˆ¶
    schedules.py          # å­¦ä¹ ç‡/ç†µç³»æ•°ç­‰è°ƒåº¦
scripts/
  train_ppo.py            # è®­ç»ƒè„šæœ¬ï¼ˆæ¼”ç¤ºå¦‚ä½•æ‹¼è£…å„æ¨¡å—ï¼‰
```

---

## rl\_core/uncertainty/scenarios.py

```python
# rl_core/uncertainty/scenarios.py
from __future__ import annotations
import numpy as np
from dataclasses import dataclass
from typing import Dict, Optional, Tuple

@dataclass
class ScenarioConfig:
    num_scenarios: int = 100                # Kï¼Œå¤šåœºæ™¯æ•°é‡
    horizon_days: int = 31                  # å›ºå®šæœˆé•¿ç¤ºä¾‹
    base_jitter_std: float = 0.12           # åŸºçº¿æŠ–åŠ¨å¼ºåº¦ï¼ˆç›¸å¯¹å€¼ï¼‰
    extreme_prob: float = 0.08              # æç«¯äº‹ä»¶å‡ºç°æ¦‚ç‡
    extreme_scale: Tuple[float, float] = (1.3, 1.8)  # æç«¯æ”¾å¤§åŒºé—´
    decay_days: int = 5                     # å‘å¸ƒå½±å“çš„è¡°å‡å¤©æ•°
    robust_buffer_pct: float = 0.10         # é²æ£’ç¼“å†²æ¯”ä¾‹ï¼ˆå¸¦å®½ä¸Šè¡Œç•™ç™½ï¼‰
    seed: Optional[int] = None

class ScenarioSampler:
    """
    å¤šåœºæ™¯ä¸ç¡®å®šæ€§é‡‡æ ·å™¨ï¼š
      - åŸºçº¿ï¼šä»å†å²åŒæœŸç»Ÿè®¡æŠ½æ ·+é«˜æ–¯æ‰°åŠ¨
      - æç«¯ï¼šä»¥ä¸€å®šæ¦‚ç‡å åŠ æ”¾å¤§äº‹ä»¶
      - å‘å¸ƒå½±å“ï¼šä¾æ® â€œç”¨æˆ·æ•°Ã—åŒ…å¤§å°Ã—æ¨¡å¼å‡å€¼Ã—æ‰°åŠ¨â€ æ³¨å…¥å¹¶æŒ‰å¤©è¡°å‡
      - é²æ£’ç¼“å†²ï¼šè¾“å‡ºæ—¶ç»Ÿä¸€ä¹˜ä»¥ (1 + buffer_pct)
    """
    def __init__(self, cfg: ScenarioConfig):
        self.cfg = cfg
        self.rng = np.random.default_rng(cfg.seed)

    def sample_baseline(self, hist_daily_mean: np.ndarray) -> np.ndarray:
        """
        hist_daily_mean: shape (31,) å†å²åŒæœŸå‡å€¼æˆ–åˆ†ä½æ•°ä½œä¸ºåŸºçº¿
        """
        assert hist_daily_mean.shape[0] == self.cfg.horizon_days
        # å¯¹æ¯æ¡åœºæ™¯åŸºçº¿åŠ ä¹˜æ€§æ‰°åŠ¨ï¼šB_t * (1 + eps), eps ~ N(0, std)
        eps = self.rng.normal(0.0, self.cfg.base_jitter_std,
                              size=(self.cfg.num_scenarios, self.cfg.horizon_days))
        base = hist_daily_mean[None, :] * (1.0 + eps)
        base = np.clip(base, a_min=0.0, a_max=None)
        return base

    def maybe_extreme(self, base: np.ndarray) -> np.ndarray:
        """
        ä»¥æ¦‚ç‡å¯¹å•ä¸ªéšæœºæ—¥è§¦å‘æç«¯æ”¾å¤§ï¼Œæ¨¡æ‹ŸèŠ‚å‡æ—¥çªå‘/çƒ­ç‚¹
        """
        S, T = base.shape
        hit = self.rng.uniform(size=S) < self.cfg.extreme_prob
        # æ¯ä¸ªå‘½ä¸­åœºæ™¯éšæœºé€‰æ‹©1~2å¤©è§¦å‘
        for i in np.where(hit)[0]:
            days = self.rng.choice(T, size=self.rng.integers(1, 3), replace=False)
            scale = self.rng.uniform(*self.cfg.extreme_scale)
            base[i, days] *= scale
        return base

    def inject_release_impact(
        self,
        scenarios: np.ndarray,
        action_day: int,
        version_profile: Dict
    ) -> np.ndarray:
        """
        å°†ä¸€æ¬¡â€œå‘å¸ƒåŠ¨ä½œâ€çš„æµé‡å½±å“æ³¨å…¥åˆ°æ‰€æœ‰åœºæ™¯
        version_profile:
          {
            "users": int,
            "pkg_mb": float,
            "pilot_ratio": float,   # æœ¬æ¬¡æ”¾é‡æ¯”ä¾‹ï¼ˆå¦‚ 0.01 é¦–æ‰¹â‰¤1%ï¼‰
            "shape_mean": float     # å†å²å½±å“å¼ºåº¦åŸºå‡†
          }
        """
        S, T = scenarios.shape
        if not (0 <= action_day < T):
            return scenarios  # éæ³•å¤©æ•°ç›´æ¥å¿½ç•¥ï¼ˆç¯å¢ƒä¼šå¦è¡Œæƒ©ç½šï¼‰

        users = version_profile["users"]
        pkg = version_profile["pkg_mb"]
        pilot = version_profile["pilot_ratio"]
        shape_mean = version_profile.get("shape_mean", 1.0)

        # åŸºç¡€å½±å“é‡ï¼ˆå•ä½å¯è¿‘ä¼¼ä¸ºâ€œç›¸å¯¹å¸¦å®½â€ï¼‰ï¼šç”¨æˆ·Ã—åŒ…Ã—å¹³å‡å½¢çŠ¶Ã—æ¯”ä¾‹
        base_influence = users * pkg * shape_mean * pilot

        # å¯¹æ¯ä¸ªåœºæ™¯æ³¨å…¥ä¹˜æ€§æ‰°åŠ¨ï¼š~ lognormal(Î¼=0, Ïƒ=0.3)
        noise = self.rng.lognormal(mean=0.0, sigma=0.3, size=S)
        amp = base_influence * noise  # æ¯ä¸ªåœºæ™¯æœ‰ä¸åŒå¹…åº¦

        # å½±å“åœ¨æ¥ä¸‹æ¥ d=0..decay_days çš„è´¡çŒ®ï¼ŒæŒ‡æ•°/çº¿æ€§è¡°å‡
        for d in range(self.cfg.decay_days):
            day = action_day + d
            if day >= T:
                break
            decay = np.exp(-0.6 * d)   # æŒ‡æ•°è¡°å‡
            scenarios[:, day] += amp * decay

        return scenarios

    def apply_robust_buffer(self, scenarios: np.ndarray) -> np.ndarray:
        return scenarios * (1.0 + self.cfg.robust_buffer_pct)

    def roll(
        self,
        hist_daily_mean: np.ndarray,
        plan_actions: Dict[int, Dict]
    ) -> np.ndarray:
        """
        ç”Ÿæˆå®Œæ•´æœˆåº¦åœºæ™¯ç»“æœï¼ˆå«æ‰€æœ‰å·²å†³ç­–çš„å‘å¸ƒåŠ¨ä½œå½±å“ï¼‰
        plan_actions: { action_day -> version_profile }
        è¿”å›ï¼šscenarios, shape=(K, horizon_days)
        """
        s = self.sample_baseline(hist_daily_mean)
        s = self.maybe_extreme(s)
        for day, prof in sorted(plan_actions.items()):
            s = self.inject_release_impact(s, day, prof)
        s = self.apply_robust_buffer(s)
        return s
```

---

## rl\_core/reward/reward.py

```python
# rl_core/reward/reward.py
from __future__ import annotations
import numpy as np
from dataclasses import dataclass
from typing import Optional, Dict

@dataclass
class RewardConfig:
    mode: str = "var_worst"     # "p95_cost" | "var_worst"
    beta_worst: float = 0.7     # æœ€ååœºæ™¯æƒé‡
    illegal_penalty: float = 2.5e6
    smooth_lambda: float = 0.0  # é‚»æ—¥å¹³æ»‘å¥–åŠ±ç³»æ•°ï¼ˆ>0é¼“åŠ±å¹³æ»‘ï¼‰
    upgrade_alpha: float = 0.0  # å‡çº§ç‡æ­£å‘å¥–åŠ±ç³»æ•°ï¼ˆ>0é¼“åŠ±æ›´å¿«å‡çº§ï¼‰

class RobustReward:
    """
    ä¸¤ç§å¸¸ç”¨ç¨³å¥ç›®æ ‡ï¼š
      - p95_costï¼šä»¥95åˆ†ä½ä½œä¸ºè®¡è´¹è¿‘ä¼¼ï¼Œè¶Šå°è¶Šå¥½ï¼ˆå–è´Ÿï¼‰
      - var_worstï¼šå¹³å‡æ–¹å·® + æœ€ååœºæ™¯æ–¹å·® * beta_worst
    å¹¶æ”¯æŒï¼š
      - è§„åˆ™è¿è§„æƒ©ç½š
      - é‚»æ—¥å¹³æ»‘å¥–åŠ±ï¼ˆå·®åˆ†å¹³æ–¹é¡¹ï¼‰
      - å‡çº§ç‡æ­£å‘å¥–åŠ±ï¼ˆç”±ç¯å¢ƒæä¾›ï¼‰
    """
    def __init__(self, cfg: RewardConfig):
        self.cfg = cfg

    @staticmethod
    def _p95(x: np.ndarray) -> float:
        return float(np.percentile(x, 95))

    @staticmethod
    def _variance(x: np.ndarray) -> float:
        return float(np.var(x))

    @staticmethod
    def _smooth_penalty(curve: np.ndarray) -> float:
        # é‚»æ—¥å·®åˆ†å¹³æ–¹å’Œ
        diffs = np.diff(curve)
        return float(np.mean(diffs * diffs))

    def __call__(
        self,
        scenarios: np.ndarray,          # (K, T)
        illegal: bool = False,
        upgrade_rate: Optional[float] = None
    ) -> float:
        K, T = scenarios.shape
        rewards = []

        if self.cfg.mode == "p95_cost":
            # å¯¹æ¯ä¸ªåœºæ™¯å–P95ï¼Œå†æ±‚å¹³å‡ï¼ˆä¹Ÿå¯ç›´æ¥å¯¹èšåˆæ›²çº¿å–P95ï¼‰
            p95_each = np.apply_along_axis(self._p95, 1, scenarios)
            base = - float(np.mean(p95_each))  # è´¹ç”¨è¶Šé«˜æƒ©ç½šè¶Šå¤§
            rewards.append(base)

        elif self.cfg.mode == "var_worst":
            # å¯¹æ¯ä¸ªåœºæ™¯æµé‡æ›²çº¿è®¡ç®—æ–¹å·®
            var_each = np.var(scenarios, axis=1)
            base = - float(np.mean(var_each))
            worst = - float(np.max(var_each)) * self.cfg.beta_worst
            rewards.extend([base, worst])

        # å¹³æ»‘å¥–åŠ±ï¼šå¯¹åœºæ™¯å¹³å‡æ›²çº¿æ±‚é‚»æ—¥å·®åˆ†æƒ©ç½šï¼ˆè´Ÿå·ï¼‰
        if self.cfg.smooth_lambda > 0:
            mean_curve = scenarios.mean(axis=0)
            rewards.append(- self.cfg.smooth_lambda * self._smooth_penalty(mean_curve))

        # å‡çº§ç‡å¥–åŠ±ï¼ˆç”±ç¯å¢ƒ/ç­–ç•¥ä¾§ç»™å…¥ï¼‰
        if self.cfg.upgrade_alpha > 0 and upgrade_rate is not None:
            rewards.append(+ self.cfg.upgrade_alpha * float(upgrade_rate))

        # éæ³•åŠ¨ä½œæƒ©ç½š
        if illegal:
            rewards.append(- self.cfg.illegal_penalty)

        return float(sum(rewards))
```

---

## rl\_core/agent/networks.py

```python
# rl_core/agent/networks.py
from __future__ import annotations
import torch
import torch.nn as nn

def mlp(sizes, activation=nn.Tanh, out_activation=nn.Identity):
    layers = []
    for i in range(len(sizes)-1):
        act = activation if i < len(sizes)-2 else out_activation
        layers += [nn.Linear(sizes[i], sizes[i+1]), act()]
    return nn.Sequential(*layers)

class CategoricalActor(nn.Module):
    """
    ç¦»æ•£åŠ¨ä½œç­–ç•¥ï¼šé€‚ç”¨äº {0:ä¸å‘å¸ƒ, 1:å‘å¸ƒ} æˆ–å¤šè‡‚é€‰æ‹©
    """
    def __init__(self, obs_dim:int, act_dim:int, hidden=(128,128)):
        super().__init__()
        self.net = mlp([obs_dim, *hidden, act_dim])

    def _distribution(self, obs):
        logits = self.net(obs)
        return torch.distributions.Categorical(logits=logits)

    def forward(self, obs, act=None):
        pi = self._distribution(obs)
        logp_a = None if act is None else pi.log_prob(act)
        return pi, logp_a

class Critic(nn.Module):
    def __init__(self, obs_dim:int, hidden=(128,128)):
        super().__init__()
        self.v = mlp([obs_dim, *hidden, 1])

    def forward(self, obs):
        return self.v(obs).squeeze(-1)

class ActorCritic(nn.Module):
    def __init__(self, obs_dim:int, act_dim:int, hidden=(128,128)):
        super().__init__()
        self.pi = CategoricalActor(obs_dim, act_dim, hidden)
        self.v  = Critic(obs_dim, hidden)

    def step(self, obs: torch.Tensor):
        with torch.no_grad():
            pi, _ = self.pi(obs)
            a = pi.sample()
            logp_a = pi.log_prob(a)
            v = self.v(obs)
        return a.numpy(), v.numpy(), logp_a.numpy()

    def act(self, obs: torch.Tensor):
        return self.step(obs)[0]
```

---

## rl\_core/agent/buffers.py

```python
# rl_core/agent/buffers.py
from __future__ import annotations
import numpy as np
import torch

class GAEBuffer:
    """
    PPOç»éªŒç¼“å†² + GAE(Î»)
    """
    def __init__(self, obs_dim, size, gamma=0.99, lam=0.95):
        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)
        self.act_buf = np.zeros(size, dtype=np.int64)
        self.adv_buf = np.zeros(size, dtype=np.float32)
        self.rew_buf = np.zeros(size, dtype=np.float32)
        self.ret_buf = np.zeros(size, dtype=np.float32)
        self.val_buf = np.zeros(size, dtype=np.float32)
        self.logp_buf = np.zeros(size, dtype=np.float32)
        self.gamma, self.lam = gamma, lam
        self.ptr, self.path_start_idx, self.max_size = 0, 0, size

    def store(self, obs, act, rew, val, logp):
        assert self.ptr < self.max_size
        self.obs_buf[self.ptr] = obs
        self.act_buf[self.ptr] = act
        self.rew_buf[self.ptr] = rew
        self.val_buf[self.ptr] = val
        self.logp_buf[self.ptr] = logp
        self.ptr += 1

    def finish_path(self, last_val=0):
        """
        ç”¨äºæ¯ä¸ª episode ç»“æŸåè®¡ç®—GAEå’Œå›æŠ¥
        """
        path_slice = slice(self.path_start_idx, self.ptr)
        rews = np.append(self.rew_buf[path_slice], last_val)
        vals = np.append(self.val_buf[path_slice], last_val)
        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]
        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)
        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]
        self.path_start_idx = self.ptr

    def get(self):
        assert self.ptr == self.max_size
        self.ptr, self.path_start_idx = 0, 0
        adv_mean, adv_std = self.adv_buf.mean(), self.adv_buf.std() + 1e-8
        self.adv_buf = (self.adv_buf - adv_mean) / adv_std
        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,
                    adv=self.adv_buf, logp=self.logp_buf)
        return {k: torch.as_tensor(v) for k, v in data.items()}

def discount_cumsum(x, discount):
    y = np.zeros_like(x, dtype=np.float32)
    c = 0.0
    for i in reversed(range(len(x))):
        c = x[i] + discount * c
        y[i] = c
    return y
```

---

## rl\_core/agent/ppo.py

```python
# rl_core/agent/ppo.py
from __future__ import annotations
import torch
import torch.nn as nn
import torch.optim as optim
from dataclasses import dataclass
from typing import Optional, Dict, Any
from .networks import ActorCritic
from .buffers import GAEBuffer

@dataclass
class PPOConfig:
    obs_dim: int
    act_dim: int = 2
    steps_per_epoch: int = 4096
    epochs: int = 50
    gamma: float = 0.99
    lam: float = 0.95
    clip_ratio: float = 0.2
    pi_lr: float = 3e-4
    vf_lr: float = 1e-3
    train_pi_iters: int = 80
    train_v_iters: int = 80
    target_kl: float = 0.015
    entropy_coef: float = 0.0
    device: str = "cpu"
    hidden: tuple = (128,128)

class PPOAgent:
    """
    æ ‡å‡† PPO-Clip å®ç°ï¼Œæ”¯æŒç†µæ­£åˆ™ä¸KLæ—©åœ
    """
    def __init__(self, cfg: PPOConfig):
        self.cfg = cfg
        self.ac = ActorCritic(cfg.obs_dim, cfg.act_dim, cfg.hidden).to(cfg.device)
        self.buf = GAEBuffer(cfg.obs_dim, cfg.steps_per_epoch, cfg.gamma, cfg.lam)
        self.pi_optimizer = optim.Adam(self.ac.pi.parameters(), lr=cfg.pi_lr)
        self.vf_optimizer = optim.Adam(self.ac.v.parameters(), lr=cfg.vf_lr)

    def update(self, data):
        obs, act, ret, adv, logp_old = data["obs"], data["act"], data["ret"], data["adv"], data["logp"]

        # ç­–ç•¥æ›´æ–°
        for i in range(self.cfg.train_pi_iters):
            pi, logp = self.ac.pi(obs, act)
            ratio = torch.exp(logp - logp_old)
            clip_adv = torch.clamp(ratio, 1 - self.cfg.clip_ratio, 1 + self.cfg.clip_ratio) * adv
            loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()

            # ç†µæ­£åˆ™ï¼ˆå¯é€‰ï¼‰
            entropy = pi.entropy().mean()
            loss_pi -= self.cfg.entropy_coef * entropy

            self.pi_optimizer.zero_grad()
            loss_pi.backward()
            nn.utils.clip_grad_norm_(self.ac.pi.parameters(), 0.5)
            self.pi_optimizer.step()

            kl = (logp_old - logp).mean().item()
            if kl > 1.5 * self.cfg.target_kl:
                break

        # ä»·å€¼å‡½æ•°æ›´æ–°
        for _ in range(self.cfg.train_v_iters):
            v = self.ac.v(obs)
            loss_v = ((v - ret)**2).mean()
            self.vf_optimizer.zero_grad()
            loss_v.backward()
            nn.utils.clip_grad_norm_(self.ac.v.parameters(), 0.5)
            self.vf_optimizer.step()

    def train_one_epoch(self, env) -> Dict[str, Any]:
        o, ep_ret, ep_len = env.reset(), 0, 0
        for t in range(self.cfg.steps_per_epoch):
            obs_t = torch.as_tensor(o, dtype=torch.float32)
            a, v, logp = self.ac.step(obs_t)
            next_o, r, d, info = env.step(int(a))
            self.buf.store(o, a, r, v, logp)
            o = next_o
            ep_ret += r; ep_len += 1

            timeout = (t == self.cfg.steps_per_epoch - 1)
            terminal = d or timeout
            if terminal:
                last_val = 0 if d else self.ac.v(torch.as_tensor(o, dtype=torch.float32)).item()
                self.buf.finish_path(last_val)
                o, ep_ret, ep_len = env.reset(), 0, 0

        data = self.buf.get()
        self.update(data)
        return {"steps": self.cfg.steps_per_epoch}
```

---

## rl\_core/envs/release\_env.py  ï¼ˆç¤ºä¾‹ç¯å¢ƒï¼Œæ–¹ä¾¿æŠŠä¸ç¡®å®šæ€§ä¸å¥–åŠ±ä¸²èµ·æ¥ï¼‰

```python
# rl_core/envs/release_env.py
from __future__ import annotations
import numpy as np
from dataclasses import dataclass
from typing import Dict, Tuple
from rl_core.uncertainty.scenarios import ScenarioSampler, ScenarioConfig
from rl_core.reward.reward import RobustReward, RewardConfig

@dataclass
class EnvConfig:
    horizon_days: int = 31
    max_actions: int = 8                 # æœ€å¤šå…è®¸ N æ¬¡å‘å¸ƒ
    illegal_days: Tuple[int,...] = ()    # å‘¨æœ«/èŠ‚å‡æ—¥ï¼ˆç¤ºä¾‹ï¼šç”¨ä¸Šæ¸¸æ—¥å†æ›¿æ¢ï¼‰
    first_batch_cap: float = 0.01        # é¦–æ‰¹â‰¤1%
    hist_level: float = 1.0              # å†å²åŸºçº¿ç¼©æ”¾
    seed: int = 42

class ReleaseEnv:
    """
    æœ€å°å¯è¿è¡Œç¯å¢ƒï¼š
      - çŠ¶æ€: [day_idx, remaining, mean, std, q25, q75, rolling_trend, å·²å‘å¸ƒè®¡æ•°]
      - åŠ¨ä½œ: 0=ä¸å‘å¸ƒ, 1=å‘å¸ƒ
      - è§„åˆ™: é¿å¼€éæ³•æ—¥; é¦–æ‰¹â‰¤1%; åŒæ—¥ä»…ä¸€æ¬¡å‘å¸ƒ
      - å¥–åŠ±: è°ƒç”¨ RobustReward (æ–¹å·®å‹/95åˆ†ä½) + è¿è§„æƒ©ç½š
      - åœºæ™¯: ScenarioSampler ç”ŸæˆKæ¡åœºæ™¯æ›²çº¿ï¼ˆå«æç«¯/ç¼“å†²/å‘å¸ƒå½±å“ï¼‰
    """
    def __init__(self,
                 env_cfg: EnvConfig,
                 sc_cfg: ScenarioConfig,
                 rw_cfg: RewardConfig):
        self.env_cfg = env_cfg
        self.sc_cfg = sc_cfg
        self.rw_cfg = rw_cfg

        self.day = 0
        self.done = False
        self.rng = np.random.default_rng(env_cfg.seed)

        self.hist_daily_mean = self._mock_hist_baseline()
        self.sampler = ScenarioSampler(sc_cfg)
        self.rewarder = RobustReward(rw_cfg)
        self.plan: Dict[int, Dict] = {}   # {day: version_profile}
        self.pilot_ratio_used = False

    def _mock_hist_baseline(self) -> np.ndarray:
        """ç¤ºä¾‹ï¼šæ„é€ ä¸€ä¸ªâ€œå·¥ä½œæ—¥é«˜/å‘¨æœ«ä½â€çš„å†å²å‡å€¼æ›²çº¿"""
        T = self.env_cfg.horizon_days
        base = np.array([1.2 if (i % 7) not in (5,6) else 0.8 for i in range(T)], dtype=np.float32)
        base *= self.env_cfg.hist_level * 1000.0  # ç¼©æ”¾åˆ°â€œç›¸å¯¹å¸¦å®½â€é‡çº§
        return base

    # ---------- Gym-like API ----------
    def reset(self):
        self.day = 0
        self.done = False
        self.plan.clear()
        self.pilot_ratio_used = False
        return self._obs()

    def step(self, action:int):
        if self.done:
            raise RuntimeError("Episode finished, call reset().")

        illegal = False
        upgrade_rate = 0.0

        # è§„åˆ™ï¼šéæ³•æ—¥ä¸å…è®¸å‘å¸ƒ
        if action == 1:
            if self.day in self.env_cfg.illegal_days:
                illegal = True
            if self.day in self.plan:
                illegal = True
            # é¦–æ‰¹â‰¤1%ï¼Œåªåœ¨é¦–æ¬¡å‘å¸ƒæ—¶æ£€æŸ¥
            pilot = 0.005 if not self.pilot_ratio_used else 0.05  # ä¸¾ä¾‹ï¼šåç»­æ”¾é‡å˜å¤§
            if (not self.pilot_ratio_used) and (pilot > self.env_cfg.first_batch_cap):
                illegal = True

            if not illegal:
                self.plan[self.day] = dict(
                    users=5_000_000,
                    pkg_mb=80.0,
                    pilot_ratio=0.005 if not self.pilot_ratio_used else 0.05,
                    shape_mean=1.0
                )
                self.pilot_ratio_used = True

        # åŸºäºå½“å‰è®¡åˆ’ç”Ÿæˆ K æ¡åœºæ™¯
        scenarios = self.sampler.roll(self.hist_daily_mean, self.plan)

        # è®¡ç®—å¥–åŠ±ï¼ˆæ­¤å¤„ä¸ºäº†æ¼”ç¤ºæŒ‰â€œæ—¥æ­¥â€å°±è¯„ä¸€æ¬¡æœˆåº¦ç¨³å¥æŒ‡æ ‡ï¼‰
        reward = self.rewarder(scenarios, illegal=illegal, upgrade_rate=upgrade_rate)

        # æ¨è¿›ä¸€å¤©
        self.day += 1
        if self.day >= self.env_cfg.horizon_days or len(self.plan) >= self.env_cfg.max_actions:
            self.done = True

        return self._obs(), reward, self.done, {"illegal": illegal}

    def _obs(self):
        # è§‚æµ‹ï¼šç®€åŒ–ä¸ºè‹¥å¹²ç»Ÿè®¡é‡ï¼ˆå¯ä¸ä½ ç¬¬ä¸€æ‰¹æ¬¡çŠ¶æ€æ‹¼æ¥ï¼‰
        recent7 = self.hist_daily_mean[max(0, self.day-7): self.day] or self.hist_daily_mean[:1]
        recent7 = np.array(recent7, dtype=np.float32)
        mean = float(self.hist_daily_mean[self.day-1]) if self.day>0 else float(self.hist_daily_mean[0])
        obs = np.array([
            self.day,                                  # day_idx
            self.env_cfg.horizon_days - self.day,      # remaining days
            float(self.hist_daily_mean.mean()),        # hist mean
            float(self.hist_daily_mean.std()),         # hist std
            float(np.percentile(self.hist_daily_mean, 25)),
            float(np.percentile(self.hist_daily_mean, 75)),
            float(recent7.mean()) if recent7.size>0 else mean,
            float(len(self.plan))                      # å·²å‘å¸ƒè®¡æ•°
        ], dtype=np.float32)
        return obs
```

---

## rl\_core/utils/seeding.py

```python
# rl_core/utils/seeding.py
import numpy as np
import random
import torch

def set_global_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
```

---

## rl\_core/utils/schedules.py

```python
# rl_core/utils/schedules.py
def linear_decay(init: float, end: float, cur_step: int, total_steps: int) -> float:
    frac = max(0.0, min(1.0, 1.0 - cur_step / float(total_steps)))
    return end + (init - end) * frac
```

---

## scripts/train\_ppo.py  ï¼ˆæ¼”ç¤ºï¼šå¦‚ä½•æŠŠä¸‰éƒ¨åˆ†æ‹¼èµ·æ¥è®­ç»ƒï¼‰

```python
# scripts/train_ppo.py
import torch
from rl_core.agent.ppo import PPOAgent, PPOConfig
from rl_core.envs.release_env import ReleaseEnv, EnvConfig
from rl_core.uncertainty.scenarios import ScenarioConfig
from rl_core.reward.reward import RewardConfig
from rl_core.utils.seeding import set_global_seed

def main():
    seed = 2025
    set_global_seed(seed)

    env_cfg = EnvConfig(
        horizon_days=31,
        max_actions=8,
        illegal_days=(5,6,12,13,19,20,26,27),   # æ¨¡æ‹Ÿå‘¨æœ«
        first_batch_cap=0.01,
        hist_level=1.0,
        seed=seed
    )
    sc_cfg = ScenarioConfig(
        num_scenarios=100,
        horizon_days=31,
        base_jitter_std=0.12,
        extreme_prob=0.08,
        extreme_scale=(1.3, 1.8),
        decay_days=5,
        robust_buffer_pct=0.10,
        seed=seed
    )
    # å¥–åŠ±å¯åˆ‡æ¢ï¼šmode="p95_cost" æˆ– "var_worst"
    rw_cfg = RewardConfig(
        mode="var_worst",
        beta_worst=0.7,
        illegal_penalty=2.5e6,
        smooth_lambda=1e-4,   # é¼“åŠ±å¹³æ»‘ï¼ˆå¯è°ƒï¼‰
        upgrade_alpha=0.0
    )

    env = ReleaseEnv(env_cfg, sc_cfg, rw_cfg)
    obs_dim = env.reset().shape[0]

    agent = PPOAgent(PPOConfig(
        obs_dim=obs_dim,
        act_dim=2,
        steps_per_epoch=4096,
        epochs=10,                 # æ¼”ç¤º
        gamma=0.99,
        lam=0.95,
        clip_ratio=0.2,
        pi_lr=3e-4,
        vf_lr=1e-3,
        train_pi_iters=80,
        train_v_iters=80,
        target_kl=0.015,
        entropy_coef=0.01,
        device="cpu",
        hidden=(128,128)
    ))

    for ep in range(agent.cfg.epochs):
        stats = agent.train_one_epoch(env)
        print(f"[Epoch {ep+1}] steps={stats['steps']}")

if __name__ == "__main__":
    main()
```

---

### è¯´æ˜ä¸å¯¹é½ç‚¹

* **ä¸ç¡®å®šæ€§å»ºæ¨¡**ï¼š`ScenarioSampler` å®ç°äº†â€œå†å²åŸºçº¿+é«˜æ–¯æ‰°åŠ¨+æç«¯äº‹ä»¶+å‘å¸ƒå½±å“+é²æ£’ç¼“å†²â€å…¨é“¾è·¯ï¼Œå¹¶æ”¯æŒå¤šåœºæ™¯ `K`ã€‚
* **å¥–åŠ±å‡½æ•°**ï¼š`RobustReward` åŒæ—¶æ”¯æŒ `p95_cost` ä¸ `var_worst` ä¸¤å¥—ç¨³å¥ç›®æ ‡ï¼›åŒ…å«**éæ³•æƒ©ç½š**ã€**å¹³æ»‘å¥–åŠ±**ä¸ï¼ˆå¯é€‰çš„ï¼‰**å‡çº§ç‡å¥–åŠ±**ã€‚
* **RL æ™ºèƒ½ä½“ï¼ˆPPOï¼‰**ï¼šæ ‡å‡† PPO-Clipï¼Œæ”¯æŒ KL æ—©åœä¸ç†µæ­£åˆ™ï¼›`train_one_epoch` å³å¯ä¸ä»»ä½• Gym-like ç¯å¢ƒè”è®­ã€‚
* **è§„åˆ™å†…åµŒ**ï¼šç¤ºä¾‹ç¯å¢ƒ `ReleaseEnv` å†…ç½®äº†â€œé¿å¼€å‘¨æœ«/èŠ‚å‡æ—¥ã€åŒæ—¥ä¸€æ¬¡ã€é¦–æ‰¹â‰¤1%â€ç­‰æ ¡éªŒï¼Œå¹¶å°†ç»“æœä¼ ç»™å¥–åŠ±å‡½æ•°è¿›è¡Œæƒ©ç½šã€‚
* **å¯æ›¿æ¢æ€§**ï¼šä½ å¯ç”¨ç”Ÿäº§ç¯å¢ƒçš„çœŸå® `Env` æ›¿æ¢ `ReleaseEnv`ï¼Œåªéœ€ä¿æŒ `reset()/step()` ä¸çŠ¶æ€/å¥–åŠ±è°ƒç”¨æ¥å£ä¸€è‡´å³å¯ã€‚

å¦‚æœä½ éœ€è¦æŠŠæœ¬æ‰¹æ¬¡ä¸**ç¬¬ä¸€æ‰¹æ¬¡ç¯å¢ƒ**å…¨é¢å¯¹æ¥ï¼Œæˆ‘å¯ä»¥ç›´æ¥å°† `ReleaseEnv` æ”¹é€ æˆä½ é‚£è¾¹çš„çŠ¶æ€/åŠ¨ä½œå®šä¹‰ï¼Œå¹¶åŠ ä¸Š**ç‰ˆæœ¬åˆ†æ‰¹æ¯”ä¾‹åŠ¨ä½œç©ºé—´**ä¸**å¯é…ç½®çº¦æŸå™¨**ã€‚

ä¸‹é¢ç»™å‡º **ç¬¬ä¸‰æ‰¹æ¬¡å®Œæ•´ä»£ç å®ç°**ï¼šè®­ç»ƒæµç¨‹ï¼ˆè®­ç»ƒè„šæœ¬ä¸è°ƒåº¦é€»è¾‘ï¼‰ã€åº”ç”¨å±‚æ¥å£ï¼ˆä¸€ä¸ªè½»é‡ Flask API + å¯¼å‡ºæ—¥å†è„šæœ¬ï¼‰ä»¥åŠå¯è§†åŒ–ä¸è¯„ä¼°æ¨¡å—ï¼ˆå¤šåœºæ™¯è¯„ä¼°ã€ä¸GAå¯¹æ¯”ã€ç»˜å›¾ä¸æŠ¥å‘Šå¯¼å‡ºï¼‰ã€‚ä»£ç ä¸å‰ä¸¤æ‰¹æ¬¡ä¿æŒæ¥å£ä¸€è‡´ï¼Œå¯ç›´æ¥ä¸ä¹‹å‰çš„ `rl_core` æ¨¡å—å¯¹æ¥ã€‚

> è¯´æ˜ï¼š
>
> * å‡è®¾å‰ä¸¤æ‰¹æ¬¡ä»£ç å·²å­˜åœ¨äº `rl_core/` ä¸‹ï¼ˆuncertainty, reward, agent, envs ç­‰ï¼‰ã€‚
> * ä»¥ä¸‹æ–‡ä»¶è·¯å¾„ä»¥é¡¹ç›®æ ¹ `rl_release_planner/` ä¸ºå‡†ã€‚
> * ä¸ºä¾¿äºè¿è¡Œï¼Œæˆ‘ä½¿ç”¨æ ‡å‡† Python åº“ä¸ matplotlibã€pandasã€Flaskã€numpyã€torch ç­‰ä¾èµ–ï¼ˆä¸ä½ çš„ `requirements.txt` ä¸€è‡´ï¼‰ã€‚

---

## ä¸€ã€è®­ç»ƒæµç¨‹ï¼ˆ`main_train.py`ï¼‰

ä¸»è®­ç»ƒè„šæœ¬ï¼ŒåŒ…å«è®­ç»ƒå¾ªç¯ã€æ¨¡å‹ä¿å­˜ã€å‘¨æœŸæ€§è¯„ä¼°ä¸æ—©åœé’©å­ã€‚

```python
# main_train.py
import os
import time
import torch
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from rl_core.utils.seeding import set_global_seed
from rl_core.envs.release_env import ReleaseEnv, EnvConfig
from rl_core.uncertainty.scenarios import ScenarioConfig
from rl_core.reward.reward import RewardConfig
from rl_core.agent.ppo import PPOAgent, PPOConfig

MODEL_DIR = Path("models")
MODEL_DIR.mkdir(exist_ok=True, parents=True)

def save_model(agent: PPOAgent, path: Path):
    torch.save(agent.ac.state_dict(), path)

def load_model(agent: PPOAgent, path: Path):
    agent.ac.load_state_dict(torch.load(path))

def main_train(seed=2025, epochs=50, eval_interval=5):
    set_global_seed(seed)

    env_cfg = EnvConfig(horizon_days=31, max_actions=8,
                        illegal_days=(), first_batch_cap=0.01, hist_level=1.0, seed=seed)
    sc_cfg = ScenarioConfig(num_scenarios=100, horizon_days=31,
                            base_jitter_std=0.12, extreme_prob=0.08,
                            extreme_scale=(1.3,1.8), decay_days=5, robust_buffer_pct=0.10, seed=seed)
    rw_cfg = RewardConfig(mode="var_worst", beta_worst=0.7,
                          illegal_penalty=2.5e6, smooth_lambda=1e-4, upgrade_alpha=0.0)

    env = ReleaseEnv(env_cfg, sc_cfg, rw_cfg)
    obs_dim = env.reset().shape[0]

    agent_cfg = PPOConfig(obs_dim=obs_dim, act_dim=2, steps_per_epoch=4096,
                          epochs=epochs, gamma=0.99, lam=0.95, clip_ratio=0.2,
                          pi_lr=3e-4, vf_lr=1e-3, train_pi_iters=80, train_v_iters=80,
                          target_kl=0.015, entropy_coef=0.01, device="cpu", hidden=(128,128))
    agent = PPOAgent(agent_cfg)

    best_reward = -1e9
    best_path = MODEL_DIR / f"best_agent_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth"

    for ep in range(epochs):
        t0 = time.time()
        stats = agent.train_one_epoch(env)
        t1 = time.time()
        print(f"[Epoch {ep+1}/{epochs}] trained steps={stats['steps']} time={(t1-t0):.1f}s")

        # å®šæœŸè¯„ä¼°
        if (ep + 1) % eval_interval == 0 or ep == epochs - 1:
            from evaluation.evaluator import Evaluator
            evaluator = Evaluator(env_cfg=env_cfg, sc_cfg=sc_cfg, rw_cfg=rw_cfg)
            metrics = evaluator.evaluate_policy(agent, n_episodes=20)
            avg_reward = metrics["avg_reward"]
            print(f"--> Eval at epoch {ep+1}: avg_reward={avg_reward:.3f} metrics={metrics}")

            # å¦‚æœè¡¨ç°æ›´å¥½åˆ™ä¿å­˜
            if avg_reward > best_reward:
                best_reward = avg_reward
                save_model(agent, best_path)
                print(f"Saved new best model to {best_path}")

    # æœ€ç»ˆæ¨¡å‹ä¿å­˜
    final_path = MODEL_DIR / f"final_agent_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth"
    save_model(agent, final_path)
    print(f"Training completed. Final model saved to {final_path}")

if __name__ == "__main__":
    main_train()
```

---

## äºŒã€è¯„ä¼°æ¨¡å—ï¼ˆ`evaluation/evaluator.py` ä¸ `evaluation/baselines.py`ï¼‰

### `evaluation/evaluator.py`

å¤šåœºæ™¯è¯„ä¼°å™¨ï¼šå°† RL ç­–ç•¥åœ¨å¤šä¸ªéšæœºåŒ–åœºæ™¯ä¸­è¿è¡Œï¼Œè¾“å‡ºç¨³å¥æ€§æŒ‡æ ‡ï¼ˆå‡å€¼æ–¹å·®ã€æœ€åæ–¹å·®ã€P95ã€è¶…é˜ˆæ¬¡æ•°ç­‰ï¼‰ï¼Œå¹¶å¯¼å‡º CSV æŠ¥è¡¨ã€‚

```python
# evaluation/evaluator.py
import numpy as np
import pandas as pd
from typing import Dict, Any
from rl_core.envs.release_env import ReleaseEnv, EnvConfig
from rl_core.uncertainty.scenarios import ScenarioConfig
from rl_core.reward.reward import RewardConfig
from tqdm import trange

class Evaluator:
    def __init__(self, env_cfg: EnvConfig, sc_cfg: ScenarioConfig, rw_cfg: RewardConfig):
        self.env_cfg = env_cfg
        self.sc_cfg = sc_cfg
        self.rw_cfg = rw_cfg

    def evaluate_policy(self, agent, n_episodes: int = 50) -> Dict[str, Any]:
        """
        è¯„ä¼° agent åœ¨å›ºå®šéšæœºç§å­/éšæœºåŒ–ä¸‹çš„ç¨³å¥æ€§èƒ½ï¼š
          - avg_reward: è¯„ä¼°æ‰€å¾—å¥–åŠ±å‡å€¼ï¼ˆepisode-levelï¼‰
          - avg_p95: å¹³å‡P95æˆæœ¬
          - worst_p95: æœ€å¤§P95æˆæœ¬ï¼ˆæœ€ååœºæ™¯ï¼‰
          - avg_var: åœºæ™¯æ–¹å·®å‡å€¼
          - worst_var: åœºæ™¯æ–¹å·®æœ€å·®
        """
        env = ReleaseEnv(self.env_cfg, self.sc_cfg, self.rw_cfg)
        p95_list, var_list, rewards = [], [], []

        for _ in trange(n_episodes, desc="Eval episodes"):
            obs = env.reset()
            done = False
            ep_reward = 0.0
            while not done:
                # è·å– action from agent (actor net)
                obs_t = np.array(obs, dtype=np.float32)
                a = agent.ac.act(torch.as_tensor(obs_t).unsqueeze(0)).item() if hasattr(agent.ac, "act") else agent.ac.step(torch.as_tensor(obs_t))[0]
                obs, r, done, info = env.step(int(a))
                ep_reward += r

            # After episode end, evaluate final plan via ScenarioSampler roll
            sampler = env.sampler
            scenarios = sampler.roll(env.hist_daily_mean, env.plan)
            p95_each = np.percentile(scenarios, 95, axis=1)
            p95_mean = float(np.mean(p95_each))
            p95_max = float(np.max(p95_each))
            var_each = np.var(scenarios, axis=1)
            var_mean = float(np.mean(var_each))
            var_max = float(np.max(var_each))

            p95_list.append((p95_mean, p95_max))
            var_list.append((var_mean, var_max))
            rewards.append(ep_reward)

        metrics = {
            "avg_reward": float(np.mean(rewards)),
            "std_reward": float(np.std(rewards)),
            "avg_p95_mean": float(np.mean([x[0] for x in p95_list])),
            "avg_p95_max": float(np.mean([x[1] for x in p95_list])),
            "avg_var_mean": float(np.mean([x[0] for x in var_list])),
            "avg_var_max": float(np.mean([x[1] for x in var_list])),
            "raw_rewards": rewards
        }
        return metrics

    def evaluate_and_export(self, agent, n_episodes=20, out_csv="evaluation_report.csv"):
        metrics = self.evaluate_policy(agent, n_episodes)
        df = pd.DataFrame([metrics])
        df.to_csv(out_csv, index=False)
        return metrics
```

### `evaluation/baselines.py`

æä¾›ä¸€ä¸ªç®€å• GA åŸºçº¿ï¼ˆè½»é‡ç‰ˆï¼‰ç”¨äºå¯¹æ¯”ã€‚æ³¨æ„ï¼šè¿™åªæ˜¯ç¤ºä¾‹åŸºçº¿ â€” çœŸå®GAå¯å¤ç”¨ä½ å…ˆå‰çš„ `GeneticOptimizer`ã€‚

```python
# evaluation/baselines.py
import numpy as np
from typing import Dict

def uniform_baseline(env):
    """
    ç®€å•åŸºçº¿ï¼šå‡åŒ€åˆ†é…æ‰¹æ¬¡åˆ°åˆæ³•å·¥ä½œæ—¥ä¸Šï¼ˆå¿½ç•¥æç«¯ä¼˜åŒ–ï¼‰
    è¿”å›ä¸€ä¸ª 'plan' å­—å…¸ {day: profile_dict}
    """
    horizon = env.env_cfg.horizon_days
    # æ‰¾åˆ°æ‰€æœ‰åˆæ³•æ—¥
    legal_days = [d for d in range(horizon) if d not in env.env_cfg.illegal_days]
    max_actions = env.env_cfg.max_actions
    chosen = legal_days[:max_actions]
    plan = {}
    for i, day in enumerate(chosen):
        plan[day] = dict(users=5_000_000, pkg_mb=80.0, pilot_ratio=0.01 if i==0 else 0.1, shape_mean=1.0)
    return plan

def evaluate_baseline(env_cfg, sc_cfg, rw_cfg, baseline_fn=uniform_baseline):
    env = ReleaseEnv(env_cfg, sc_cfg, rw_cfg)
    # use baseline plan directly
    plan = baseline_fn(env)
    sampler = env.sampler
    scenarios = sampler.roll(env.hist_daily_mean, plan)
    res = {
        "p95_mean": float(np.mean(np.percentile(scenarios, 95, axis=1))),
        "p95_max": float(np.max(np.percentile(scenarios, 95, axis=1))),
        "var_mean": float(np.mean(np.var(scenarios, axis=1))),
        "var_max": float(np.max(np.var(scenarios, axis=1))),
    }
    return res
```

---

## ä¸‰ã€å¯è§†åŒ–æ¨¡å—ï¼ˆ`evaluation/visualization.py`ï¼‰

ç»˜åˆ¶è®­ç»ƒæ›²çº¿ã€åœºæ™¯æµé‡æ›²çº¿ã€å‘å¸ƒæ—¥å†å¯¼å‡ºå›¾è¡¨ç­‰ã€‚

```python
# evaluation/visualization.py
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from typing import Dict, Any

def plot_scenarios(scenarios: np.ndarray, title="Scenario flows", out_png=None):
    """
    scenarios: (K, T)
    """
    K, T = scenarios.shape
    fig, ax = plt.subplots(figsize=(10,5))
    for i in range(min(K, 50)):   # æœ€å¤šç”»50æ¡ä»¥å…è¿‡å¯†
        ax.plot(range(T), scenarios[i], alpha=0.15)
    mean_curve = scenarios.mean(axis=0)
    ax.plot(range(T), mean_curve, color='black', linewidth=2, label='Mean curve')
    ax.set_title(title)
    ax.set_xlabel("Day")
    ax.set_ylabel("Relative Traffic")
    ax.legend()
    if out_png:
        fig.savefig(out_png, dpi=150)
    plt.show()

def plot_calendar(plan: Dict[int, Dict], horizon=31, out_png=None):
    """
    ç®€å•ç»˜åˆ¶å‘å¸ƒæ—¥å†ï¼šxè½´ä¸ºå¤©ï¼Œyä¸ºæ˜¯å¦å‘å¸ƒ/æ‰¹æ¬¡é‡
    plan: {day: profile}
    """
    arr = np.zeros(horizon)
    for d, prof in plan.items():
        arr[d] = prof.get("pilot_ratio", 0.0) + prof.get("pkg_mb",0.0) * 0.0 + 1.0
    fig, ax = plt.subplots(figsize=(12,3))
    ax.bar(range(horizon), arr, color='tab:blue')
    ax.set_title("Release Calendar (bars indicate release days)")
    ax.set_xlabel("Day of month")
    ax.set_ylabel("Release flag / proxy size")
    if out_png:
        fig.savefig(out_png, dpi=150)
    plt.show()

def plot_eval_metrics(metrics: Dict[str, Any], out_png=None):
    keys = [k for k in metrics.keys() if k.startswith("avg_") or k.endswith("_p95") or k.endswith("_var")]
    vals = [metrics[k] for k in keys]
    fig, ax = plt.subplots(figsize=(8,4))
    ax.bar(keys, vals)
    ax.set_title("Evaluation metrics summary")
    ax.set_xticklabels(keys, rotation=45, ha='right')
    if out_png:
        fig.savefig(out_png, dpi=150, bbox_inches='tight')
    plt.show()
```

---

## å››ã€åº”ç”¨å±‚æ¥å£ï¼ˆ`app/api.py` ä¸ `app/export_calendar.py`ï¼‰

### `app/api.py`ï¼ˆFlask å¿«é€Ÿ APIï¼Œæä¾›æ¨¡å‹é¢„æµ‹ã€å¯¼å‡ºè®¡åˆ’ï¼‰

```python
# app/api.py
from flask import Flask, request, jsonify, send_file
import torch
import json
from pathlib import Path
from rl_core.envs.release_env import EnvConfig
from rl_core.uncertainty.scenarios import ScenarioConfig
from rl_core.reward.reward import RewardConfig
from rl_core.agent.ppo import PPOAgent, PPOConfig
from evaluation.evaluator import Evaluator
import numpy as np

MODEL_DIR = Path("models")
LATEST_MODEL = sorted(MODEL_DIR.glob("*.pth"))[-1] if any(MODEL_DIR.glob("*.pth")) else None

app = Flask(__name__)

def _load_agent(model_path: str):
    # åˆ›å»º agent skeletonï¼Œå¹¶åŠ è½½å‚æ•°
    # éœ€è¦ä¸è®­ç»ƒæ—¶ä¿æŒ obs_dim, act_dim ä¸€è‡´
    env_cfg = EnvConfig()
    sc_cfg = ScenarioConfig()
    rw_cfg = RewardConfig()
    dummy_env = ReleaseEnv(env_cfg, sc_cfg, rw_cfg)
    obs_dim = dummy_env.reset().shape[0]
    agent_cfg = PPOConfig(obs_dim=obs_dim, act_dim=2, steps_per_epoch=4096, epochs=1)
    agent = PPOAgent(agent_cfg)
    agent.ac.load_state_dict(torch.load(model_path, map_location="cpu"))
    return agent

@app.route("/predict_plan", methods=["POST"])
def predict_plan():
    """
    è¯·æ±‚ç¤ºä¾‹:
      { "model": "best_agent_xxx.pth", "context": { ... } }
    è¿”å›:
      { "plan": {day: profile}, "metrics": {...} }
    """
    data = request.json
    model_name = data.get("model", None) or str(LATEST_MODEL)
    if model_name is None:
        return jsonify({"error": "No model available"}), 400
    model_path = Path(model_name)
    if not model_path.exists():
        model_path = MODEL_DIR / model_name
    if not model_path.exists():
        return jsonify({"error": f"Model {model_name} not found"}), 404

    agent = _load_agent(str(model_path))
    # create env and run greedy policy to produce plan
    env_cfg = EnvConfig()
    sc_cfg = ScenarioConfig()
    rw_cfg = RewardConfig()
    env = ReleaseEnv(env_cfg, sc_cfg, rw_cfg)

    obs = env.reset()
    done = False
    while not done:
        obs_t = np.array(obs, dtype=np.float32)
        a = agent.ac.act(torch.as_tensor(obs_t).unsqueeze(0)).item()
        obs, r, done, info = env.step(int(a))

    plan = env.plan
    # Evaluate final plan for reporting
    sampler = env.sampler
    scenarios = sampler.roll(env.hist_daily_mean, plan)
    p95_each = np.percentile(scenarios, 95, axis=1)
    metrics = {
        "p95_mean": float(np.mean(p95_each)),
        "p95_max": float(np.max(p95_each)),
        "var_mean": float(np.mean(np.var(scenarios, axis=1))),
        "var_max": float(np.max(np.var(scenarios, axis=1)))
    }
    return jsonify({"plan": {int(k): v for k,v in plan.items()}, "metrics": metrics})

@app.route("/health", methods=["GET"])
def health():
    return jsonify({"alive": True, "model_loaded": bool(LATEST_MODEL)})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
```

### `app/export_calendar.py`ï¼ˆå¯¼å‡º CSVï¼‰

```python
# app/export_calendar.py
import csv
from datetime import datetime
from typing import Dict

def export_plan_to_csv(plan: Dict[int, Dict], filename="release_calendar.csv"):
    """
    plan: {day_idx: profile}
    è¾“å‡ºåˆ—: day, users, pkg_mb, pilot_ratio, shape_mean
    """
    with open(filename, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["day", "users", "pkg_mb", "pilot_ratio", "shape_mean"])
        for day, prof in sorted(plan.items()):
            writer.writerow([int(day), prof.get("users"), prof.get("pkg_mb"),
                             prof.get("pilot_ratio"), prof.get("shape_mean")])
    return filename
```

---

## äº”ã€ç¤ºä¾‹è¯„ä¼° & å¯è§†åŒ–è„šæœ¬ï¼ˆ`main_eval.py`ï¼‰

```python
# main_eval.py
from pathlib import Path
import torch
from rl_core.envs.release_env import EnvConfig
from rl_core.uncertainty.scenarios import ScenarioConfig
from rl_core.reward.reward import RewardConfig
from rl_core.agent.ppo import PPOAgent, PPOConfig
from evaluation.evaluator import Evaluator
from evaluation.visualization import plot_eval_metrics, plot_scenarios, plot_calendar
import numpy as np

def load_agent_from_path(path):
    # assume obs_dim matches
    env_cfg = EnvConfig()
    sc_cfg = ScenarioConfig()
    rw_cfg = RewardConfig()
    env = ReleaseEnv(env_cfg, sc_cfg, rw_cfg)
    obs_dim = env.reset().shape[0]
    agent_cfg = PPOConfig(obs_dim=obs_dim)
    agent = PPOAgent(agent_cfg)
    agent.ac.load_state_dict(torch.load(path, map_location="cpu"))
    return agent

def main_eval(model_path=None):
    model_dir = Path("models")
    if model_path is None:
        models = sorted(model_dir.glob("*.pth"))
        if not models:
            raise FileNotFoundError("No models found")
        model_path = models[-1]
    print("Loading model:", model_path)
    agent = load_agent_from_path(str(model_path))

    env_cfg = EnvConfig()
    sc_cfg = ScenarioConfig()
    rw_cfg = RewardConfig()

    evaluator = Evaluator(env_cfg, sc_cfg, rw_cfg)
    metrics = evaluator.evaluate_policy(agent, n_episodes=30)
    print("Evaluation metrics:", metrics)

    # one example rollout to visualize
    env = ReleaseEnv(env_cfg, sc_cfg, rw_cfg)
    obs = env.reset()
    done = False
    while not done:
        obs_t = np.array(obs, dtype=np.float32)
        a = agent.ac.act(torch.as_tensor(obs_t).unsqueeze(0)).item()
        obs, r, done, info = env.step(int(a))

    scenarios = env.sampler.roll(env.hist_daily_mean, env.plan)
    plot_scenarios(scenarios, title="Example rollout scenarios", out_png="examples_scenarios.png")
    plot_calendar(env.plan, horizon=env_cfg.horizon_days, out_png="examples_calendar.png")
    plot_eval_metrics(metrics, out_png="examples_metrics.png")
    print("Saved example visualizations to disk")

if __name__ == "__main__":
    main_eval()
```

---

## å…­ã€å•å…ƒæµ‹è¯•ï¼ˆ`tests/test_training_eval.py` ç®€å•ç¤ºä¾‹ï¼‰

ç¡®ä¿è®­ç»ƒ loop / eval èƒ½è·‘é€šï¼ˆè¿è¡Œé€Ÿåº¦å¯è°ƒæ•´ï¼‰ã€‚

```python
# tests/test_training_eval.py
import pytest
from rl_core.envs.release_env import EnvConfig
from rl_core.uncertainty.scenarios import ScenarioConfig
from rl_core.reward.reward import RewardConfig
from rl_core.agent.ppo import PPOAgent, PPOConfig
from rl_core.utils.seeding import set_global_seed

def test_env_and_agent_quick():
    set_global_seed(123)
    env_cfg = EnvConfig(horizon_days=10, max_actions=4, illegal_days=(5,6), first_batch_cap=0.01, hist_level=1.0, seed=123)
    sc_cfg = ScenarioConfig(num_scenarios=10, horizon_days=10, seed=123)
    rw_cfg = RewardConfig(mode="var_worst", beta_worst=0.5)
    from rl_core.envs.release_env import ReleaseEnv
    env = ReleaseEnv(env_cfg, sc_cfg, rw_cfg)
    obs_dim = env.reset().shape[0]
    agent_cfg = PPOConfig(obs_dim=obs_dim, act_dim=2, steps_per_epoch=128, epochs=1)
    agent = PPOAgent(agent_cfg)
    # run a brief training epoch to check integration
    stats = agent.train_one_epoch(env)
    assert "steps" in stats
```

---

## ä¸ƒã€è¿è¡Œä¸éƒ¨ç½²å»ºè®®

1. å¼€å‘/è°ƒè¯•é˜¶æ®µï¼šåœ¨ CPU ä¸Šå°è§„æ¨¡è·‘ `main_train.py`ï¼ˆepochs=å°‘é‡ï¼Œsteps\_per\_epochå°ï¼‰ä»¥ç¡®ä¿é€»è¾‘é“¾è·¯æ— è¯¯ã€‚
2. è®­ç»ƒé˜¶æ®µï¼šåœ¨ GPU ä¸Šè·‘ `main_train.py`ï¼Œå¹¶æŠŠ `steps_per_epoch` / `epochs` æ‰©å¤§åˆ°ç”Ÿäº§çº§åˆ«ã€‚
3. æ¨¡å‹éƒ¨ç½²ï¼šå°†è®­ç»ƒå¥½çš„ `.pth` æ¨¡å‹æ”¾åˆ° `models/` ä¸‹ï¼Œè¿è¡Œ `app/api.py` æä¾›é¢„æµ‹æ¥å£ã€‚
4. è¯„ä¼°é˜¶æ®µï¼šå®šæœŸä½¿ç”¨ `main_eval.py` è¿›è¡Œå›å½’è¯„ä¼°ï¼Œä¸ GA åŸºçº¿æ¯”è¾ƒã€‚
5. ç›‘æ§ä¸çº¿ä¸Šæ ¡æ­£ï¼šæŠŠçœŸå®å‘å¸ƒæ•°æ®å›å†™ä¸ºæ–°çš„å†å²æ ·æœ¬ç”¨äºåç»­åœ¨çº¿å­¦ä¹ æˆ–å‘¨æœŸé‡è®­ã€‚

---

## å…«ã€ç»“æŸè¯­

ä»¥ä¸Šä»£ç å®ç°äº†**ç¬¬ä¸‰æ‰¹æ¬¡**æ‰€éœ€çš„è®­ç»ƒæµç¨‹ã€è¯„ä¼°/å¯¹æ¯”ã€å¯è§†åŒ–ä¸ä¸€ä¸ªè½»é‡çš„æœåŠ¡æ¥å£ã€‚å®ƒä¸å‰ä¸¤æ‰¹æ¬¡ï¼ˆç¯å¢ƒã€è§„åˆ™ã€ä¸ç¡®å®šæ€§ã€å¥–åŠ±ã€PPO agentï¼‰å¯ä»¥æ— ç¼è¡”æ¥ã€‚

å¦‚æœä½ éœ€è¦ï¼š

* æˆ‘å¯ä»¥æŠŠæ•´ä¸ªé¡¹ç›®æ‰“åŒ…ä¸ºå¯è¿è¡Œçš„ GitHub ä»“åº“ï¼ˆåŒ…å« `dockerfile`, CI æµ‹è¯•è„šæœ¬ï¼‰ï¼›
* æˆ–è€…æˆ‘å¯ä»¥å°† GA çš„å®Œæ•´å®ç°ï¼ˆä½ ä¹‹å‰çš„ `GeneticOptimizer`ï¼‰å¹¶å…¥ `evaluation/baselines.py` ä½œä¸ºæ›´å¼ºçš„ baselineï¼›
* ä¹Ÿå¯ä»¥æŠŠ Flask API æ‰©å±•ä¸ºå¸¦è®¤è¯ã€å¹¶å‘é™åˆ¶çš„ç”Ÿäº§æœåŠ¡ã€‚

è¦ä¸è¦æˆ‘ç°åœ¨æŠŠè¿™äº›æ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªå¯ä¸‹è½½çš„ ZIP åŒ…ï¼Œæˆ–æŠŠæŸäº›æ–‡ä»¶å±•å¼€æˆæ›´è¯¦ç»†çš„å®ç°ï¼ˆä¾‹å¦‚ï¼šGA å®Œæ•´å®ç°ã€å‰ç«¯ dashboard ç¤ºä¾‹ï¼‰ï¼Ÿ
